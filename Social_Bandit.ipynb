{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erfunmirzaei/Social_Bandit/blob/main/Social_Bandit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmxTbpiBG2w3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import gym\n",
        "from scipy.stats import t\n",
        "from statistics import mean\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXGaCzywZb8N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73ZyrJoSEr0j"
      },
      "source": [
        "# Implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDAUQ9OOE00W"
      },
      "source": [
        "## Rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcicFR7yDweH"
      },
      "outputs": [],
      "source": [
        "class MultinomialReward():\n",
        "    def __init__(self, rewards, probs):\n",
        "        self.rewards = rewards\n",
        "        self.probs = probs\n",
        "        self.mean = np.array(probs) @ np.array(rewards)        \n",
        "\n",
        "    def get_reward(self, t):\n",
        "        return random.choices(self.rewards, weights = self.probs, k = 1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou6K5LJuDweI"
      },
      "outputs": [],
      "source": [
        "class DeteriministicReward():\n",
        "    def __init__(self, reward):\n",
        "        self.reward = reward\n",
        "        self.mean = reward\n",
        "        \n",
        "    def get_reward(self, t):\n",
        "        return self.reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwIgeTb0DweJ"
      },
      "outputs": [],
      "source": [
        "class GaussianReward():\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def get_reward(self, t):\n",
        "        return np.random.normal(loc=self.mean, scale=self.std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eliQKoxtDweJ"
      },
      "outputs": [],
      "source": [
        "class Students_t_distReward():\n",
        "    def __init__(self,dof, mean, std):\n",
        "        self.dof = dof\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def get_reward(self, t):\n",
        "        return t.rvs(df = self.dof, loc=self.mean, scale=self.std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqY-W0PZMG1d"
      },
      "outputs": [],
      "source": [
        "class MultinomialReward_NonStationary():\n",
        "    def __init__(self, rewards, probs, switch_t):\n",
        "        self.rewards = rewards\n",
        "        self.probs = probs\n",
        "        self.mean = np.array(probs) @ np.array(rewards)  \n",
        "        self.switch_t = switch_t\n",
        "\n",
        "    def get_reward(self, t):\n",
        "        if t < self.switch_t:\n",
        "            return random.choices(self.rewards, weights = self.probs, k = 1)[0]\n",
        "        \n",
        "        else:\n",
        "            return random.choices(self.rewards[::-1], weights = self.probs, k = 1)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11fHtdR3E5X_"
      },
      "source": [
        "## N-Armed Bandit Environments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHflIod8DweK"
      },
      "outputs": [],
      "source": [
        "class MutliArmedBanditEnvironment():\n",
        "    def __init__(self, rewards, episode_max_length, id, container=None):\n",
        "        state_space = gym.spaces.Discrete(1)\n",
        "        action_space = gym.spaces.Discrete(len(rewards))\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = state_space\n",
        "        self.state = None\n",
        "        self.id = id\n",
        "        self.agents = {}\n",
        "        \n",
        "        if container != None:\n",
        "            container.register_environment(self)\n",
        "        self.arms_rewards = rewards # consits all rewards method\n",
        "        self.episode_max_length = episode_max_length\n",
        "        self.state = {\n",
        "            'length': 0,\n",
        "            'last_action': None\n",
        "        }\n",
        "\n",
        "    def add_agent(self, agent_id):\n",
        "        self.agents_last_choice[agent_id] = -1 \n",
        "        \n",
        "    def calculate_reward(self, action):\n",
        "        return self.arms_rewards[action].get_reward(self.state['length'])\n",
        "\n",
        "    def terminated(self):\n",
        "        return self.state['length'] >= self.episode_max_length\n",
        "\n",
        "    def observe(self):\n",
        "        return {}\n",
        "\n",
        "    def n_actions(self):\n",
        "        return self.action_space.n\n",
        "\n",
        "    def next_state(self, action):\n",
        "        self.state['length'] += 1\n",
        "        self.state['last_action'] = action\n",
        "    \n",
        "    def get_info(self):\n",
        "        return {} \n",
        "\n",
        "    def update_selected(self, action, agent_id):\n",
        "        if agent_id == -1:\n",
        "            return\n",
        "        self.agents_last_choice[agent_id] = action\n",
        "\n",
        "    def step(self, action, agent_id):\n",
        "        reward = self.calculate_reward(action)\n",
        "        observation = self.observe()\n",
        "        info = self.get_info()\n",
        "        self.next_state(action)\n",
        "        done = self.terminated()\n",
        "        self.update_selected(action, agent_id) #agent_id == -1 ==> do nothing\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.state['length'] = 0\n",
        "        self.state['last_action'] = None\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "\n",
        "    def close(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAQ_lcEVUjfk"
      },
      "outputs": [],
      "source": [
        "class SocialMutliArmedBanditEnvironment():\n",
        "    def __init__(self, rewards, episode_max_length, container = None):\n",
        "        state_space = gym.spaces.Discrete(1)\n",
        "        action_space = gym.spaces.Discrete(len(rewards))\n",
        "\n",
        "        self.action_space = action_space\n",
        "        self.observation_space = state_space\n",
        "        \n",
        "        if container != None:\n",
        "            container.register_environment(self)\n",
        "\n",
        "        self.arms_rewards = rewards\n",
        "        self.agents_last_choice = {}\n",
        "        self.episode_max_length = episode_max_length\n",
        "        # self.optimal_action = np.argmax(R_mean)\n",
        "\n",
        "        self.state = {\n",
        "            'length': 0,\n",
        "            'last_action': None\n",
        "        }\n",
        "    \n",
        "    def add_agent(self, agent_id):\n",
        "        self.agents_last_choice[agent_id] = -1 \n",
        "     \n",
        "    def calculate_reward(self, action):\n",
        "        return self.arms_rewards[action].get_reward(self.state['length'])\n",
        "    \n",
        "    def update_selected(self, action, agent_id):\n",
        "        if agent_id == -1:\n",
        "            return\n",
        "        self.agents_last_choice[agent_id] = action\n",
        "        \n",
        "    def step(self, action, agent_id):\n",
        "        reward = self.calculate_reward(action)\n",
        "        observation = self.observe()\n",
        "        info = self.get_info()\n",
        "        self.next_state(action)\n",
        "        done = self.terminated()\n",
        "        self.update_selected(action, agent_id) #agent_id == -1 ==> do nothing\n",
        "        return observation, reward, done, info\n",
        "    \n",
        "    def terminated(self):\n",
        "        return self.state['length'] >= self.episode_max_length\n",
        "\n",
        "    def observe(self):\n",
        "        return {}\n",
        "    \n",
        "    def n_actions(self):\n",
        "        return self.action_space.n\n",
        "\n",
        "    def next_state(self, action):\n",
        "        self.state['length'] += 1\n",
        "        self.state['last_action'] = action\n",
        "    \n",
        "    def get_info(self):\n",
        "        return {} \n",
        "\n",
        "    def config(self):\n",
        "        self.agents_last_choice = []\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state['length'] = 0\n",
        "        self.state['last_action'] = None\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "\n",
        "    def close(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vTZafAEE-8H"
      },
      "source": [
        "# Agents "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learner Agents"
      ],
      "metadata": {
        "id": "syUMOjD4LRpi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JLeNiNNDweL"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AgentBase:\n",
        "    def __init__(self, id, environment=None):\n",
        "        self.id = id\n",
        "        self.environment = environment\n",
        "        self.environment.add_agent(id)\n",
        "\n",
        "    def set_environment(self, env):\n",
        "        self.environment = env\n",
        "\n",
        "    @abstractmethod\n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        # in this method, you MUST call the `step` method of \n",
        "        # the environment and observe the results and return them like:\n",
        "        # return observation, reward, done, info\n",
        "        pass\n",
        "    \n",
        "    def utility_function(self,reward):\n",
        "        if reward >= 0 :\n",
        "            u = reward ** self.alpha\n",
        "        else:\n",
        "            u =  -1 * self.gamma * ((-reward)** self.beta)\n",
        "        return u "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HYwd6-7DweL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#Add Temperature\n",
        "\n",
        "class EpsilonGreedyAgent(AgentBase): \n",
        "    def __init__(self, id, environment, epsilon = 0.1,epsilon_decay = 1,lr = None,lr_decay = 1,alpha = 1,beta = 1,gamma = 1,optimistic = False):\n",
        "        super(EpsilonGreedyAgent, self).__init__(id, environment)\n",
        "        n_actions = self.environment.n_actions()\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.Q = np.zeros((n_actions,1))     #action value fuction\n",
        "        self.N = np.zeros((n_actions,1))     #number of doing an action \n",
        "\n",
        "    def get_number_actions(self):\n",
        "        return self.N\n",
        "\n",
        "    def update(self,action,utility):\n",
        "        self.epsilon = self.epsilon * self.epsilon_decay\n",
        "        self.N[action] += 1\n",
        "        if self.lr is not None:\n",
        "            self.Q[action] += (utility - self.Q[action]) * self.lr\n",
        "            self.lr = self.lr * self.lr_decay \n",
        "        else:\n",
        "            self.Q[action] += (utility - self.Q[action])/self.N[action] \n",
        "\n",
        "    def select_action(self):\n",
        "        n_actions = self.environment.n_actions()\n",
        "        rand = np.random.rand()\n",
        "        if rand < self.epsilon:\n",
        "            action = np.random.choice(n_actions)\n",
        "        else:\n",
        "            action = np.argmax(self.Q)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        action = self.select_action()\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        u = self.utility_function(r)\n",
        "        self.update(action,u)\n",
        "        \n",
        "        return r, action , u\n",
        "\n",
        "    def get_Q(self):\n",
        "        return self.Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZYTv8J3VAMO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class UCBAgent(AgentBase):\n",
        "    def __init__(self, id, environment, c, lr = None, lr_decay = 1,alpha = 1,beta = 1,gamma = 1):\n",
        "        super(UCBAgent, self).__init__(id, environment)\n",
        "        n_actions = self.environment.n_actions()\n",
        "        self.c = c\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.Q = np.zeros((n_actions,1))                 #action value function(expected reward) for each arm\n",
        "        self.N = np.zeros((n_actions,1))                 #number of doing each arm\n",
        "        self.trial = 0                                          #number of total trials \n",
        "        self.UCB = np.inf * np.ones((n_actions,1))      #Upper confidence bound for each arm\n",
        "        \n",
        "    def select_action(self):        \n",
        "        action = np.random.choice(np.flatnonzero(self.UCB == self.UCB.max()))\n",
        "        return action \n",
        "\n",
        "    def update(self, action, u):        \n",
        "        # update action values\n",
        "        self.trial += 1\n",
        "        self.N[action] = self.N[action] + 1\n",
        "        if self.lr is not None:\n",
        "            self.Q[action] += self.lr*(u-self.Q[action])\n",
        "            self.lr = self.lr * self.lr_decay\n",
        "        else:\n",
        "            self.Q[action] += (u-self.Q[action]) / self.N[action]\n",
        "\n",
        "        self.UCB = self.Q + self.c * np.sqrt(np.log(self.trial)/(self.N + 1e-9))\n",
        "\n",
        "        \n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        action = self.select_action()\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        u = self.utility_function(r)\n",
        "        self.update(action, u)\n",
        "        return r, action, u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38CnYMSoByxk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GradientBanditAgent(AgentBase): \n",
        "    def __init__(self, id, environment, lr = 0.1,lr_decay = 1,alpha = 1,beta = 1,gamma = 1,optimistic = False, Baseline_lr = None):\n",
        "        super(GradientBanditAgent, self).__init__(id, environment)\n",
        "        n_actions = self.environment.n_actions()\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.R_lr = Baseline_lr\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.avg_rew = 0   \n",
        "        self.N = 0      \n",
        "        self.H = np.zeros((n_actions))\n",
        "        self.P = (1/n_actions)*np.ones((n_actions))\n",
        "\n",
        "    def update(self,action,utility):\n",
        "        self.lr = self.lr * self.lr_decay\n",
        "        self.N += 1\n",
        "        if self.R_lr is not None:\n",
        "            self.avg_rew += (utility - self.avg_rew) * self.R_lr\n",
        "        else:\n",
        "            self.avg_rew += (utility - self.avg_rew)/self.N\n",
        "\n",
        "        for i in range(len(self.H)):\n",
        "            if i == action :\n",
        "                self.H[i] += self.lr *(utility - self.avg_rew)*(1-self.P[i])\n",
        "\n",
        "            else:\n",
        "                self.H[i] -=  self.lr *(utility - self.avg_rew)*(self.P[i])\n",
        "\n",
        "        for i in range(len(self.P)):\n",
        "            self.P[i] = np.exp(self.H[i]) / np.sum(np.exp(self.H))\n",
        "\n",
        "    def get_all(self):\n",
        "        return self.H,self.P\n",
        "\n",
        "    def select_action(self):\n",
        "        n_actions = self.environment.n_actions()\n",
        "        action = int(np.random.choice(list(range(n_actions)), size=1,p=self.P))       \n",
        "        return action \n",
        "\n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        action = self.select_action()\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        #print(obs, r, d, i)\n",
        "        u = self.utility_function(r)\n",
        "        self.update(action,u)\n",
        "        #self.environment.render()\n",
        "        return r, action , u\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HNl0mID_YrT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ActorCriticAgent(AgentBase): \n",
        "    def __init__(self, id, environment, lr = 0.1,lr_decay = 1,alpha = 1,beta = 1,gamma = 1,optimistic = False):\n",
        "        super(ActorCriticAgent, self).__init__(id, environment)\n",
        "        n_actions = self.environment.n_actions()\n",
        "        self.lr = lr\n",
        "        self.lr_decay = lr_decay\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.N = 0      \n",
        "        self.Q = np.zeros((n_actions))     #action value fuction\n",
        "        self.H = np.zeros((n_actions))\n",
        "        self.P = (1/n_actions)*np.ones((n_actions))\n",
        "\n",
        "    def update(self,action,utility):\n",
        "        self.lr = self.lr * self.lr_decay\n",
        "        self.N += 1\n",
        "        if self.lr is not None:\n",
        "            self.Q[action] += (utility - self.Q[action]) * self.lr\n",
        "            self.lr = self.lr * self.lr_decay \n",
        "        else:\n",
        "            self.Q[action] += (utility - self.Q[action])/self.N[action] \n",
        "\n",
        "        for i in range(len(self.H)):\n",
        "            if i == action :\n",
        "                self.H[i] += self.lr *(utility - self.Q[i])*(1-self.P[i])\n",
        "\n",
        "            else:\n",
        "                self.H[i] -=  self.lr *(utility - self.Q[i])*(self.P[i])\n",
        "\n",
        "        for i in range(len(self.P)):\n",
        "            self.P[i] = np.exp(self.H[i]) / np.sum(np.exp(self.H))\n",
        "\n",
        "    def get_all(self):\n",
        "        return self.H,self.P\n",
        "\n",
        "    def select_action(self):\n",
        "        n_actions = self.environment.n_actions()\n",
        "        action = int(np.random.choice(list(range(n_actions)), size=1,p=self.P))  \n",
        "        return action\n",
        "\n",
        "    def take_action(self) -> (object, float, bool, object):\n",
        "        action = self.select_action()\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        #print(obs, r, d, i)\n",
        "        u = self.utility_function(r)\n",
        "        self.update(action,u)\n",
        "        #self.environment.render()\n",
        "        return r, action , u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej-BOALdDr4Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from math import sqrt\n",
        "\n",
        "class ThompsonSamplingAgent(AgentBase):\n",
        "    def __init__(self, id, environment, alpha = 1,beta = 1,gamma = 1):\n",
        "        super(ThompsonSamplingAgent, self).__init__(id, environment)\n",
        "        n_actions = self.environment.n_actions()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.stds = list(1000000000 * np.ones((n_actions,1)))  #stds of estimated gaussian distributions\n",
        "        self.means = list( np.zeros((n_actions,1)))            #means of estimated gaussian distributions\n",
        "\n",
        "    def get_samples(self,means,stds):\n",
        "        samples = [np.random.normal(means[i],stds[i]) for i in range(len(means))]\n",
        "        return samples\n",
        "\n",
        "    def update(self,reward,action):\n",
        "        new_std = sqrt( 1 / ((1/self.stds[action]**2) + 1) )\n",
        "        new_mean = (reward + (self.means[action] / self.stds[action] ** 2)) / ((1/self.stds[action]**2) + 1) \n",
        "        return new_mean, new_std\n",
        "    \n",
        "    def select_action(self):\n",
        "        samples = self.get_samples(self.means,self.stds)\n",
        "        action = np.argmax(samples)\n",
        "        return action \n",
        "\n",
        "    def take_action(self):# -> (object, float, bool, object):\n",
        "        action = self.select_action()\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        u = self.utility_function(r)\n",
        "        self.means[action], self.stds[action] = self.update(u,action)\n",
        "        #print(obs, r, d, i)\n",
        "        #self.environment.render()\n",
        "        return r,action, u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTOmpSzeEpZG"
      },
      "source": [
        "## Coded Agents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp8MRB72EpZH"
      },
      "outputs": [],
      "source": [
        "class AlwaysBestAgent(AgentBase):\n",
        "    def __init__(self,id, environment, alpha = 1,beta = 1,gamma = 1):\n",
        "        super(AlwaysBestAgent, self).__init__(id, environment)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.EU = np.array([self.utility_function(r.mean) for r in self.environment.arms_rewards])\n",
        "        self.bestaction = np.random.choice(np.flatnonzero(self.EU == self.EU.max()))\n",
        "        # self.regret = []\n",
        "        \n",
        "    def take_action(self):\n",
        "        obs, r, d, i = self.environment.step(self.bestaction, self.id)\n",
        "        #self.r = self.r + self.environment.rewards[np.argmax([r.m for r in self.environment.rewards])].m - self.environment.rewards[indx].mean\n",
        "        #self.regret.append(self.r)\n",
        "        return r, self.bestaction, self.utility_function(r)\n",
        "\n",
        "    \n",
        "class PercentBestAgent(AgentBase):\n",
        "    def __init__(self, id, environment, p, alpha = 1,beta = 1,gamma = 1, increase = 0, limit = 1, zero_at = False):\n",
        "        super(PercentBestAgent, self).__init__(id, environment)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.p = p\n",
        "        self.increase = increase\n",
        "        self.zero_at = zero_at\n",
        "        self.limit = limit\n",
        "        self.EU = np.array([self.utility_function(r.mean) for r in self.environment.arms_rewards])\n",
        "        self.bestaction = np.random.choice(np.flatnonzero(self.EU == self.EU.max()))\n",
        "        self.n_actions = self.environment.n_actions()\n",
        "        # self.regret = []\n",
        "        \n",
        "        \n",
        "    def take_action(self):\n",
        "        if np.random.rand() < self.p:\n",
        "            action = self.bestaction\n",
        "        else:\n",
        "            action = np.random.choice(self.n_actions)\n",
        "        \n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "\n",
        "        #self.r = self.r + self.environment.rewards[np.argmax([r.m for r in self.environment.rewards])].m - self.environment.rewards[indx].m\n",
        "        #self.regret.append(self.r)\n",
        "\n",
        "        self.p = self.p + self.increase\n",
        "        if self.p > self.limit:\n",
        "            self.p = self.limit\n",
        "            self.increase = 0\n",
        "            \n",
        "        if self.zero_at:\n",
        "            if self.p > 1:\n",
        "                self.p = 0\n",
        "                self.increase = 0\n",
        "        return r, action, self.utility_function(r)\n",
        "    \n",
        "class SecondBestAgent(AgentBase):\n",
        "    def __init__(self,id, environment, alpha = 1,beta = 1,gamma = 1):\n",
        "        super(SecondBestAgent, self).__init__(id, environment)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.EU = np.array([self.utility_function(r.mean) for r in self.environment.arms_rewards])\n",
        "        SecondEU = np.array([x for x in self.EU if x < max(self.EU)])\n",
        "        self.secondbestaction = np.random.choice(np.flatnonzero(self.EU == SecondEU.max()))\n",
        "        # self.regret = []\n",
        "\n",
        "    def take_action(self):\n",
        "        obs, r, d, i = self.environment.step(self.secondbestaction, self.id)\n",
        "        return r, self.secondbestaction, self.utility_function(r)\n",
        "    \n",
        "class AlwaysWorstAgent(AgentBase):\n",
        "    def __init__(self,id, environment, alpha = 1,beta = 1,gamma = 1):\n",
        "        super(AlwaysWorstAgent, self).__init__(id, environment)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.EU = np.array([self.utility_function(r.mean) for r in self.environment.arms_rewards])\n",
        "        self.worstaction = np.random.choice(np.flatnonzero(self.EU == self.EU.min()))\n",
        "        # self.regret = []\n",
        "        \n",
        "    def take_action(self):\n",
        "        obs, r, d, i = self.environment.step(self.worstaction, self.id)\n",
        "        return r, self.worstaction, self.utility_function(r)\n",
        "    \n",
        "    def utility_function(self,reward):\n",
        "        if reward >= 0 :\n",
        "            u = reward ** self.alpha\n",
        "        else:\n",
        "            u =  -1 * self.gamma * ((-reward)** self.beta)\n",
        "        return u \n",
        "        \n",
        "class AlwaysRandomAgent(AgentBase):\n",
        "    def __init__(self,id, environment, alpha = 1,beta = 1,gamma = 1):\n",
        "        super(AlwaysRandomAgent, self).__init__(id, environment)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta \n",
        "        self.gamma = gamma\n",
        "        self.n_actions = self.environment.n_actions()\n",
        "        # self.regret = []\n",
        "\n",
        "    def take_action(self):\n",
        "        action = np.random.choice(self.n_actions)\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        return r, action, self.utility_function(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "4XEXTLpjLleb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test1_Learner Agents "
      ],
      "metadata": {
        "id": "TweQfzExrl2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Multinomial_Rewards = {  '2arm_Hard' : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 1000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.8,0.2], switch_t = 1000)] ,\n",
        "                       \n",
        "                         '2arm_Med'  : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 1000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.5,0.5], switch_t = 1000)],\n",
        "                       \n",
        "                         '2arm_Easy' : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 1000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.1,0.9], switch_t = 1000)]} "
      ],
      "metadata": {
        "id": "PMq15arWraXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SocialMutliArmedBanditEnvironment(Multinomial_Rewards['2arm_Easy'], 10000)"
      ],
      "metadata": {
        "id": "uyTbvX0YrbYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(10000), np.array([0.9] * 1000 + [0.1] * 9000), 'b')\n",
        "ax.plot(range(10000), np.array([0.1] * 1000 + [0.9] * 9000),'r')\n",
        "\n",
        "ax.set_ylabel('True Mean Reward',rotation = 90)\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend([\"arm 1\", \"arm 2\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b-DNurXrro9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret1 = np.zeros((10000,50))\n",
        "perc_opt_act1 = np.zeros((10000,50))\n",
        "average_rewards1 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9    \n",
        "    agent1 = EpsilonGreedyAgent('1', env, epsilon = 0.1, lr = 0.1)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent1.take_action()\n",
        "\n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 1000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret1[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret1[t,r] = 1000 * 0.9 + (t+1 - 1000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act1[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards1[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "x6JlebW2rxr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret2 = np.zeros((10000,50))\n",
        "perc_opt_act2 = np.zeros((10000,50))\n",
        "average_rewards2 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    exp_opt_act = 0.9\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    \n",
        "    agent2 = UCBAgent('1', env, c =2, lr = 0.1)\n",
        "    # agent2 = UCBBanditAgent('1',env, c = 2, lr = 0.1)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent2.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 1000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret2[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else:\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret2[t,r] = 1000 * 0.9 + (t+1 - 1000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act2[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards2[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "5Tt4RIQsr3FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret3 = np.zeros((10000,50))\n",
        "perc_opt_act3 = np.zeros((10000,50))\n",
        "average_rewards3 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9\n",
        "\n",
        "    agent3 = ThompsonSamplingAgent('1', env)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent3.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 1000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret3[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret3[t,r] = 1000 * 0.9 + (t+1 - 1000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act3[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards3[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "JFw-LTu9OBfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret4 = np.zeros((10000,50))\n",
        "perc_opt_act4 = np.zeros((10000,50))\n",
        "average_rewards4 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9\n",
        "    \n",
        "    agent4 = GradientBanditAgent('5', env)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent4.take_action()\n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 1000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret4[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "        \n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret4[t,r] = 1000 * 0.9 + (t+1 - 1000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act4[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards4[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "N7b2NhoAONKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env.reset()\n",
        "regret5 = np.zeros((10000,50))\n",
        "perc_opt_act5 = np.zeros((10000,50))\n",
        "average_rewards5 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    exp_opt_act = 0.9\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "\n",
        "    agent5 = ActorCriticAgent('1', env, lr = 0.1)\n",
        "\n",
        "    # agent4 = gradientAgent('1',env, alpha = 0.1, Baseline_lr = 0.01)\n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent5.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 1000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret5[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "        \n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret5[t,r] = 1000 * 0.9 + (t+1 - 1000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act5[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards5[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "Dr59PDr1PRZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "som1 = np.std(perc_opt_act1,axis = 1) / np.sqrt(perc_opt_act1.shape[1])\n",
        "mean1 = np.mean(perc_opt_act1,axis = 1)\n",
        "\n",
        "som2 = np.std(perc_opt_act2,axis = 1) / np.sqrt(perc_opt_act2.shape[1])\n",
        "mean2 = np.mean(perc_opt_act2,axis = 1)\n",
        "\n",
        "som3 = np.std(perc_opt_act3,axis = 1) / np.sqrt(perc_opt_act3.shape[1])\n",
        "mean3 = np.mean(perc_opt_act3,axis = 1)\n",
        "\n",
        "som4 = np.std(perc_opt_act4,axis = 1) / np.sqrt(perc_opt_act4.shape[1])\n",
        "mean4 = np.mean(perc_opt_act4,axis = 1)\n",
        "\n",
        "som5 = np.std(perc_opt_act5,axis = 1) / np.sqrt(perc_opt_act5.shape[1])\n",
        "mean5 = np.mean(perc_opt_act5,axis = 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(10000), mean1, 'b')\n",
        "ax.fill_between(range(10000), (mean1-som1), (mean1+som1), color='b', alpha=.1)\n",
        "ax.plot(range(10000), mean2,'r')\n",
        "ax.fill_between(range(10000), (mean2-som2), (mean2+som2), color='r', alpha=.1)\n",
        "ax.plot(range(10000), mean3, 'g')\n",
        "ax.fill_between(range(10000), (mean3-som3), (mean3+som3), color='g', alpha=.1)\n",
        "ax.plot(range(10000), mean4, 'black')\n",
        "ax.fill_between(range(10000), (mean4-som4), (mean4+som4), color='black', alpha=.1)\n",
        "ax.plot(range(10000), mean5, 'gray')\n",
        "ax.fill_between(range(10000), (mean5-som5), (mean5+som5), color='gray', alpha=.1)\n",
        "ax.set_ylabel('% Optimal action',rotation = 90)\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend([\"Eps-greedy\", \"UCB\",\"Thompson\", \"Policy-gradient\", \"Actor-Critic\" ])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BmFvZBbGPV85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test2_Coded Agents"
      ],
      "metadata": {
        "id": "6BJbW-zkPpy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Multinomial_Rewards = {  '2arm_Hard' : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 9000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.8,0.2], switch_t = 9000)] ,\n",
        "                       \n",
        "                         '2arm_Med'  : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 9000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.5,0.5], switch_t = 9000)],\n",
        "                       \n",
        "                         '2arm_Easy' : [MultinomialReward_NonStationary([1,0],[0.9,0.1], switch_t = 9000),\n",
        "                                        MultinomialReward_NonStationary([1,0],[0.1,0.9], switch_t = 9000)]} "
      ],
      "metadata": {
        "id": "q51SZpupPySf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SocialMutliArmedBanditEnvironment(Multinomial_Rewards['2arm_Easy'], 10000)"
      ],
      "metadata": {
        "id": "2nEDZRmJPySg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(10000), np.array([0.9] * 9000 + [0.1] * 1000), 'b')\n",
        "ax.plot(range(10000), np.array([0.1] * 9000 + [0.9] * 1000),'r')\n",
        "\n",
        "ax.set_ylabel('True Mean Reward',rotation = 90)\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend([\"arm 1\", \"arm 2\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nLLVx-6XPySh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret1 = np.zeros((10000,50))\n",
        "perc_opt_act1 = np.zeros((10000,50))\n",
        "average_rewards1 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9    \n",
        "    agent1 = AlwaysBestAgent('1', env)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent1.take_action()\n",
        "\n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 9000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret1[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret1[t,r] = 9000 * 0.9 + (t+1 - 9000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act1[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards1[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "2hhYmOSlPySi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret2 = np.zeros((10000,50))\n",
        "perc_opt_act2 = np.zeros((10000,50))\n",
        "average_rewards2 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    exp_opt_act = 0.9\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    \n",
        "    agent2 = PercentBestAgent('1', env, p = 0.8)\n",
        "    # agent2 = UCBBanditAgent('1',env, c = 2, lr = 0.1)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent2.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 9000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret2[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else:\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret2[t,r] = 9000 * 0.9 + (t+1 - 9000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act2[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards2[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "XeyjShsvPySi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret3 = np.zeros((10000,50))\n",
        "perc_opt_act3 = np.zeros((10000,50))\n",
        "average_rewards3 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9\n",
        "\n",
        "    agent3 = SecondBestAgent('1', env)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent3.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 9000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret3[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret3[t,r] = 9000 * 0.9 + (t+1 - 9000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act3[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards3[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "4ABXTesHPySj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.reset()\n",
        "regret4 = np.zeros((10000,50))\n",
        "perc_opt_act4 = np.zeros((10000,50))\n",
        "average_rewards4 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.9\n",
        "    \n",
        "    agent4 = AlwaysWorstAgent('4', env)\n",
        "    \n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent4.take_action()\n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 9000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret4[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "        \n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret4[t,r] = 9000 * 0.9 + (t+1 - 9000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act4[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards4[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "ai2i7lGzPySk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "env.reset()\n",
        "regret5 = np.zeros((10000,50))\n",
        "perc_opt_act5 = np.zeros((10000,50))\n",
        "average_rewards5 = np.zeros((10000,50))\n",
        "\n",
        "for r in tqdm(range(50)):\n",
        "    exp_opt_act = 0.9\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "\n",
        "    agent5 = AlwaysRandomAgent('1', env)\n",
        "\n",
        "    # agent4 = gradientAgent('1',env, alpha = 0.1, Baseline_lr = 0.01)\n",
        "    for t in range(10000):\n",
        "        rew , act, _ = agent5.take_action()\n",
        "        \n",
        "        sum_rewards += rew\n",
        "        \n",
        "        if t < 9000:\n",
        "            opt_act_count += int(act == 0)\n",
        "            regret5[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "        \n",
        "        else :\n",
        "            opt_act_count += int(act == 1)\n",
        "            exp_opt_act = 0.9\n",
        "            regret5[t,r] = 9000 * 0.9 + (t+1 - 9000)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act5[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards5[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env.reset()"
      ],
      "metadata": {
        "id": "waOPxDZfPySl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "som1 = np.std(perc_opt_act1,axis = 1) / np.sqrt(perc_opt_act1.shape[1])\n",
        "mean1 = np.mean(perc_opt_act1,axis = 1)\n",
        "\n",
        "som2 = np.std(perc_opt_act2,axis = 1) / np.sqrt(perc_opt_act2.shape[1])\n",
        "mean2 = np.mean(perc_opt_act2,axis = 1)\n",
        "\n",
        "som3 = np.std(perc_opt_act3,axis = 1) / np.sqrt(perc_opt_act3.shape[1])\n",
        "mean3 = np.mean(perc_opt_act3,axis = 1)\n",
        "\n",
        "som4 = np.std(perc_opt_act4,axis = 1) / np.sqrt(perc_opt_act4.shape[1])\n",
        "mean4 = np.mean(perc_opt_act4,axis = 1)\n",
        "\n",
        "som5 = np.std(perc_opt_act5,axis = 1) / np.sqrt(perc_opt_act5.shape[1])\n",
        "mean5 = np.mean(perc_opt_act5,axis = 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(10000), mean1, 'b')\n",
        "ax.fill_between(range(10000), (mean1-som1), (mean1+som1), color='b', alpha=.1)\n",
        "ax.plot(range(10000), mean2,'r')\n",
        "ax.fill_between(range(10000), (mean2-som2), (mean2+som2), color='r', alpha=.1)\n",
        "ax.plot(range(10000), mean3, 'g')\n",
        "ax.fill_between(range(10000), (mean3-som3), (mean3+som3), color='g', alpha=.1)\n",
        "ax.plot(range(10000), mean4, 'black')\n",
        "ax.fill_between(range(10000), (mean4-som4), (mean4+som4), color='black', alpha=.1)\n",
        "ax.plot(range(10000), mean5, 'gray')\n",
        "ax.fill_between(range(10000), (mean5-som5), (mean5+som5), color='gray', alpha=.1)\n",
        "ax.set_ylabel('% Optimal action',rotation = 90)\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend([\"AlwaysBest\", \"BestPercent\",\"SecondBest\", \"AlwaysWorst\", \"AlwasRandom\" ])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GkPOQkjmPySl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test"
      ],
      "metadata": {
        "id": "WT-G_muSJW2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent id -1 is equivalant to None"
      ],
      "metadata": {
        "id": "BDVpDK72u6-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MostFrequentSocialStrategy():\n",
        "    def __init__(self):\n",
        "    \n",
        "    def select_action(self, env, social_information, selecting_policy):\n",
        "        agent_id = np.random.choice(len(selecting_policy),p = selecting_policy)        \n",
        "        return env.agents_last_choice[agent_id], agent_id"
      ],
      "metadata": {
        "id": "YdKWVicoNAiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PreferenceBasedSocialAgent(AgentBase):\n",
        "\n",
        "    def __init__(self, id, env, alpha, ilm):\n",
        "        super(PreferenceBasedSocialAgent, self).__init__(id, env)\n",
        "        self.alpha = np.ones(len(self.preference))*alpha\n",
        "        self.ilm = ilm \n",
        "        self.preferences = np.zeros(len(self.environment.agents_last_choice))+0.0001\n",
        "        # uniform initialization, (Others can be done, ourself 1 the others zero)\n",
        "        self.social_information = np.zeros((len(self.environment.agents_last_choice),self.environment.n_actions()))\n",
        "        self.selecting_policy = np.ones(len(self.environment.agents_last_choice))/len(self.environment.agents_last_choice)\n",
        "        self.total_mean_reward = 0 \n",
        "        self.t = 0\n",
        "        self.preferences_history  = {[] for i in range(len(self.preference))}\n",
        "        self.selecting_history = []\n",
        "\n",
        "    def update(self, u, action):\n",
        "        self.t += 1\n",
        "        self.total_mean_reward += (u - self.total_mean_reward)/self.t\n",
        "        same = []\n",
        "        for i in range(len(self.preferences)):\n",
        "            self.social_information[i][self.environment.agents_last_choice[i]] += 1\n",
        "            if self.environment.agents_last_choice[i]== action:\n",
        "                same.append[i]\n",
        "        if self.id not in same and self.individual.select_action() == action:\n",
        "            same.append(self.id)\n",
        "        \n",
        "        self.preferences -= self.alpha *  (u - self.total_mean_reward) * self.selecting_policy\n",
        "        for inedx in same:\n",
        "            self.preferences[index] += self.alpha * (u = self.total_mean_reward)\n",
        "        # self.preference = np.round(self.preference,2)\n",
        "        # d = np.sum(np.exp(self.preference))\n",
        "        # mask = np.ones(len(self.preference))*alpha\n",
        "        # self.preference = self.preference - mask * (Ri - self.mean_reward) * np.exp(self.preference)/d\n",
        "        # for index in same:\n",
        "        #     self.preference[index] = self.preference[index] + alpha * (Ri - self.mean_reward)\n",
        "\n",
        "        p = np.exp(self.preferences - np.max(self.preferences))\n",
        "        self.selecting_policy = p / p.sum(axis=0)\n",
        "        for i in range(len(self.preferences)):\n",
        "            self.preferences_history[i].append(self.selecting_policy[i])\n",
        "        # p =  np.round(np.exp(np.round(self.preferences,2)),2)\n",
        "        # self.selecting_policy = p/np.sum(p)\n",
        "        # for j in range(len(p)):\n",
        "        #     if np.isnan(p[j]):\n",
        "        #         p[j] = 1\n",
        "\n",
        "    def select_action(self, sls, ind_mode):\n",
        "        action, agent_id = sls.select_action(agent_id, self.environment, self.social_information, self.selecting_policy) \n",
        "        if agent_id == self.id or self.ind_mode:\n",
        "            self.selecting_history.append[self.id]\n",
        "            action = self.ilm.select_action()\n",
        "        else:\n",
        "            self.selecting_history.append[agent_id]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def take_action(self, sls, ind_mode = False):\n",
        "        action = self.select_action(sls, ind_mode)\n",
        "        obs, r, d, i = self.environment.step(action, self.id)\n",
        "        u = self.utility_function(r)\n",
        "        self.update(u,action)\n",
        "        return r,action, u"
      ],
      "metadata": {
        "id": "zKt85cNFsHS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MetaSocialLearnerAgent(AgentBase):\n",
        "\n",
        "    def __init__(self, id, env, ilm, slm, sls):\n",
        "        super(MetaSocialLearnerAgent, self).__init__(id, env)\n",
        "        self.ilm = ilm \n",
        "        self.slm = slm\n",
        "        self.sls = sls \n",
        "\n",
        "    def update(self):\n",
        "        # Novelty\n",
        "        # Volatileness\n",
        "        # Uncertainty\n",
        "    \n",
        "    def take_action(self):\n",
        "        r, action, u = self.slm.take_action(self.sls, ind_mode)\n",
        "        self.means[action], self.stds[action] = self.update(u,action)\n",
        "        return r,action, u"
      ],
      "metadata": {
        "id": "ARUX2qwyPhJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class population():\n",
        "\n",
        "    def __init__(self, ilms, slms, slss):"
      ],
      "metadata": {
        "id": "cSxgFpiXPrKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SocialBanditLearning():\n",
        "    def __init__(self, pop):\n",
        "\n",
        "\n",
        "    def\n",
        "\n",
        "n_trials, n_reps, "
      ],
      "metadata": {
        "id": "0iPOr6AvNYsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Det_Rew1 = [DeteriministicReward(0.8),DeteriministicReward(0.2)]\n",
        "Det_Rew2 = [DeteriministicReward(0.5),DeteriministicReward(0.2)]\n",
        "\n",
        "env1 = SocialMutliArmedBanditEnvironment(Det_Rew1, 1000)\n",
        "env2 = SocialMutliArmedBanditEnvironment(Det_Rew2, 1000)"
      ],
      "metadata": {
        "id": "mQmET_dGJY_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env1.reset()\n",
        "regret1 = np.zeros((1000,200))\n",
        "perc_opt_act1 = np.zeros((1000,200))\n",
        "average_rewards1 = np.zeros((1000,200))\n",
        "\n",
        "for r in tqdm(range(200)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.8    \n",
        "    agent1 = EpsilonGreedyAgent('1', env1, epsilon = 0.1)\n",
        "    \n",
        "    for t in range(1000):\n",
        "        rew , act, _ = agent1.take_action()\n",
        "\n",
        "        sum_rewards += rew\n",
        "        opt_act_count += int(act == 0)\n",
        "        regret1[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act1[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards1[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env1.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmCh8P5VKGM4",
        "outputId": "4908b26b-ed02-4219-a9ec-c5d83c00d359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:05<00:00, 37.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env2.reset()\n",
        "regret2 = np.zeros((1000,200))\n",
        "perc_opt_act2 = np.zeros((1000,200))\n",
        "average_rewards2 = np.zeros((1000,200))\n",
        "\n",
        "for r in tqdm(range(200)):\n",
        "    sum_rewards = 0\n",
        "    opt_act_count = 0\n",
        "    exp_opt_act = 0.5    \n",
        "    agent2 = EpsilonGreedyAgent('1', env2, epsilon = 0.1, lr = 0.1)\n",
        "    \n",
        "    for t in range(1000):\n",
        "        rew , act, _ = agent1.take_action()\n",
        "\n",
        "        sum_rewards += rew\n",
        "        opt_act_count += int(act == 0)\n",
        "        regret2[t,r] = (t+1)*exp_opt_act - sum_rewards\n",
        "\n",
        "        perc_opt_act2[t,r] = opt_act_count / (t+1)\n",
        "        average_rewards2[t,r] = sum_rewards / (t+1)\n",
        "\n",
        "    env2.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fF7YAUGtLfUG",
        "outputId": "57290901-82fe-4e34-b2f0-2206e53f4625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:05<00:00, 37.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "som1 = np.std(perc_opt_act1,axis = 1) / np.sqrt(perc_opt_act1.shape[1])\n",
        "mean1 = np.mean(perc_opt_act1,axis = 1)\n",
        "\n",
        "som2 = np.std(perc_opt_act2,axis = 1) / np.sqrt(perc_opt_act2.shape[1])\n",
        "mean2 = np.mean(perc_opt_act2,axis = 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(range(1000), mean1, 'b')\n",
        "ax.fill_between(range(1000), (mean1-som1), (mean1+som1), color='b', alpha=.1)\n",
        "ax.plot(range(1000), mean2,'r')\n",
        "ax.fill_between(range(1000), (mean2-som2), (mean2+som2), color='r', alpha=.1)\n",
        "ax.set_ylabel('% Optimal action',rotation = 90)\n",
        "ax.set_xlabel('Steps')\n",
        "ax.legend([\"0.8-0.2\" , \"0.5-0.2\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "ExUYsxX6LSqr",
        "outputId": "4ffe52cb-c3d6-47d5-ac1e-7eaeeaf7f35e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZhcZZnw/+99au09ne7O2iELCZCArGF1AWRQdEaQZVh0FB0ZHcV3ZNRX4eX3Uwdl4J1xAUVHcQHRGVDBAYQIIuu4gcGELRASIJDOnu703rWe+/3jOaerurvSqSRd6aT7/lxXX111tnpOLc99nvWIqmKMMcYM5413AowxxuyfLEAYY4wpyQKEMcaYkixAGGOMKckChDHGmJKi452AsdLc3Kzz5s0b72QYY8wB5emnn96uqi2l1k2YADFv3jyWL18+3skwxpgDioi8vrN1VsVkjDGmJAsQxhhjSrIAYYwxpqQJ0wZhjDEA2WyWtrY2UqnUeCdlv5JMJmltbSUWi5W9jwUIY8yE0tbWRl1dHfPmzUNExjs5+wVVpb29nba2NubPn1/2flbFZIyZUFKpFE1NTRYciogITU1Nu12qsgBhjJlwLDiMtCfviQUIY4wxJVmAANJp6Ooa71QYYyaSBx54gEMPPZSFCxdy/fXXj1j/xhtvcPrpp3PMMcdw5JFHsmzZspLH6ejo4Mwzz2TRokWceeaZ7NixY8Q2K1eu5OSTT+bwww/nyCOP5Gc/+9mYnIMFCKC/H/r6xjsVxpiJIp/Pc/nll/PrX/+aVatWcfvtt7Nq1aoh23zlK1/hwgsvZMWKFdxxxx184hOfKHms66+/njPOOIM1a9ZwxhlnlAw21dXV3Hbbbbzwwgs88MADXHHFFXR2du71eViAMMaYMfbUU0+xcOFCFixYQDwe5+KLL+aee+4Zso2I0N3dDUBXVxezZs0qeax77rmHSy+9FIBLL72Uu+++e8Q2hxxyCIsWLQJg1qxZTJs2jW3btu31eVg3V2PMhHXFFbBy5dge8+ij4YYbRt9mw4YNzJkzZ/B5a2srTz755JBtvvSlL/GOd7yDb33rW/T19fHb3/625LG2bNnCzJkzAZgxYwZbtmwZ9bWfeuopMpkMBx98cBlnMzorQRhjzDi4/fbb+dCHPkRbWxvLli3jAx/4AL7vj7qPiIzaG2nTpk184AMf4JZbbsHz9j57txKEMWbC2tWVfqXMnj2b9evXDz5va2tj9uzZQ7b54Q9/yAMPPADAySefTCqVYvv27Xz+859nxYoVzJo1i2XLljF9+nQ2bdrEzJkz2bRpE9OmTSv5mt3d3fz1X/811157LSeddNKYnIeVIAKq450CY8xEcfzxx7NmzRpee+01MpkMd9xxB2efffaQbQ466CAefvhhAF588UVSqRQtLS3ccsstrFy5crBX09lnn82Pf/xjAH784x9zzjnnjHi9TCbDueeeywc/+EEuuOCCMTsPCxDGGDPGotEoN910E+985ztZvHgxF154IYcffjhf+MIXuPfeewH42te+xve//32OOuooLrnkEm699daS1UdXXnklDz30EIsWLeK3v/0tV155JQDLly/nsssuA+DnP/85TzzxBLfeeitHH300Rx99NCvHoPFFtIKXziJyFnAjEAF+oKrXD1s/F/gR0AJ0AH+nqm0icjrwjaJNDwMuVtWRzfeBpUuX6p7eMGjHDtfVdVgJ0BhzAHrxxRdZvHjxeCdjv1TqvRGRp1V1aantK1aCEJEI8G3gXcAS4BIRWTJss68Ct6nqkcA1wHUAqvqoqh6tqkcDbwf6gd9UKq3GGGNGqmQV0wnAWlV9VVUzwB3A8MqzJcAjweNHS6wHuAD4tar2VyqhNm2LMcaMVMkAMRtYX/S8LVhW7BngvODxuUCdiDQN2+Zi4PZSLyAiHxWR5SKyfCwGhRhjjCkY70bqzwKnisgK4FRgA5APV4rITOBNwIOldlbVm1V1qaoubWlp2RfpNcaYSaOS4yA2AHOKnrcGywap6kaCEoSI1ALnq2rxBCIXAv+tqtkKptMYY0wJlSxB/BlYJCLzRSSOqyq6t3gDEWkWkTANV+F6NBW7hJ1ULxljjKmsigUIVc0Bn8RVD70I/FxVXxCRa0QkHDFyGrBaRF4GpgPXhvuLyDxcCeTxSqVxaHr3xasYYyaLXU33feutt9LS0jI4buEHP/hByeOk02kuuugiFi5cyIknnsi6detGbLN+/XpOP/10lixZwuGHH86NN944JudQ0ak2VHUZsGzYsi8UPb4TuHMn+65jZKO2Mcbs98Lpvh966CFaW1s5/vjjOfvss1myZGhP/4suuoibbrpp1GP98Ic/pLGxkbVr13LHHXfw+c9/fsT9HqLRKF/72tc49thj6enp4bjjjuPMM88c8Xq7a7wbqY0xZsIpZ7rvchVP933BBRfw8MMPM3yA88yZMzn22GMBqKurY/HixWzYsGHEsXaXTdaHjYMwZsIap/m+y5nuG+Cuu+7iiSee4JBDDuEb3/jGkH1KHSsajdLQ0EB7ezvNzc0lX3vdunWsWLGCE088cXfOqiQrQRhjzDh4z3vew7p163j22Wc588wzB0sJe6O3t5fzzz+fG264gfr6+r0+npUgjDET1zjN913OdN9NTYUxwZdddhmf+9znALj66qu5//77AXev6fBYra2t5HI5urq6huwbymaznH/++bz//e/nvPPOG7F+T1gJwhhjxlg5031v2rRp8PG99947OInetddey8qVKwdnYy2e7vvOO+/k7W9/+4hZX1WVj3zkIyxevJhPf/rTY3YeFiCMMWaMlTPd9ze/+U0OP/xwjjrqKL75zW9y6623ljzWRz7yEdrb21m4cCFf//rXB7vMbty4kXe/+90A/P73v+cnP/kJjzzyyGC32fB+EnujotN970t7M913Vxf09EBr6xgnyhizz9l03zu330z3bYwx5sBmASIwQQpSxhgzZixAGGMmnIlSdT6W9uQ9sQBhjJlQkskk7e3tFiSKqCrt7e0kk8nd2s/GQWAjqY2ZSFpbW2lra8NuIjZUMpmkdTd74liAMMZMKLFYjPnz5493MiYEq2IyxhhTkgUIY4wxJVmAMMYYU5IFCGOMMSVZgDDGGFOSBYiAdZk2xpihLEAYY4wpqaIBQkTOEpHVIrJWRK4ssX6uiDwsIs+KyGMi0lq07iAR+Y2IvCgiq0RkXuXSWakjG2PMgatiAUJEIsC3gXcBS4BLRGTJsM2+CtymqkcC1wDXFa27Dfh3VV0MnABsrVRajTHGjFTJEsQJwFpVfVVVM8AdwDnDtlkCPBI8fjRcHwSSqKo+BKCqvaraX4lEDgzArbfCyy9X4ujGGHPgqmSAmA2sL3reFiwr9gwQ3jz1XKBORJqAQ4BOEfmliKwQkX8PSiRDiMhHRWS5iCzf03lXenrgn/4J/vjHPdrdGGMmrPFupP4scKqIrABOBTYAedwcUW8N1h8PLAA+NHxnVb1ZVZeq6tKWlpY9SkDY/mC9mIwxZqhKBogNwJyi563BskGqulFVz1PVY4Crg2WduNLGyqB6KgfcDRxbiURagDDGmNIqGSD+DCwSkfkiEgcuBu4t3kBEmkUkTMNVwI+K9p0iImGx4O3Aqkok0nowGWNMaRULEMGV/yeBB4EXgZ+r6gsico2InB1sdhqwWkReBqYD1wb75nHVSw+LyHOAAN+vRDrDAOH7lTi6McYcuCp6PwhVXQYsG7bsC0WP7wTu3Mm+DwFHVjJ9YFVMxhizM+PdSD3uLEAYY0xpFiCK2yAsShhjzCALEGEJIpeHTZvGNzHGGLMfsQARBoi8by3VxhhTxAJEECBkoN+qmIwxpogFiOJGat+3IGGMMQELEMUBQtUChDHGBCxADA6UUytBGGNMEQsQVoIwxpiSLECE4yB8tQBhjDFFLEBYCcIYY0qyAGG9mIwxpiQLEIMBwqqYjDGm2C5ncxWRNwNfAuYG2wugqrqgsknbNwbbIHJ5CxDGGFOknOm+fwj8M/A07nagE8qQEoRVMRljzKByAkSXqv664ikZJ9ZIbYwxpZUTIB4VkX8Hfgmkw4Wq+peKpWofGqxiCuPC/hYgwvSIuBKON+mbjYwx+0g5AeLE4P/SomWKu0/0hKEw/lVMqtDd7YLA1q1QXw9dXW6diFs/cyZUVVmgMMZU3C4DhKqevi8SMt40nOl7vAJETw+0t7v/IhCLQUcHxOMuGIQliLY2SCRg+nS3bU8PTJ3qgkkkMj5pN8ZMSOX0YmoAvgi8LVj0OHCNqnZVMmH7kogytXsdtPfB3Ln77oXzediypVByiMddZj/kNnfBdu3t0NgIdXXQ1wdvvOECRjLpShtbtrh96+pcALEShjFmL5VTxfQj4HngwuD5B4BbgPN2taOInAXcCESAH6jq9cPWzw2O3wJ0AH+nqm3BujzwXLDpG6p6dhlp3SMi8IX7T4SH4rB5c6VeZqQwY6+qcsEByGzcTv6Ht5BYtQLvheddKSK0eDF85jNwxhkQjcLvfw8LFsDs2S6IdHS4v0QCqqvdH0A2Czt2uGDS0jL4WsbsNb/oRlsiharQsNRrDmiiu6hSEZGVqnr0rpaV2C8CvAycCbQBfwYuUdVVRdv8ArhPVX8sIm8HPqyqHwjW9apqbbknsnTpUl2+fHm5mw8RjSq5fHDFvX07NDXt0XF2RRXUV7xsGnp7Yds2qKtDU2l673+M6n/7EpGNbSN3fMtb4OWXobMTMhmYMcO1RaxY4da/7W3wt38L557rfpT9/S4ohKUIz3NBI5Nxj6dOdT/qeNxtn89Dba1LYCZjJZB9LZdzf9Go+zx2VVVY/JstJxPO5wuZd7hP8cDQMIPP5wvpEXHpUXXPi7cLO0t0dsJLL7kSree5v2jUfX/mzCksK05rOu32r6py1aj5vPseJhLuvMPtMxn3PFwWibj9Pa8QlGIx91y1sC58f3x/6PbhctXCsYp7LeZybrvwPQi3KX7PRQrbhb+bWMyds+e55+F2xe+377v9wsfhvuGyaLRwnp7njqlaSEt4HsPTNEZE5GlVXVpqXTkliAEReYuq/i442JuBgTL2OwFYq6qvBvvdAZwDrCraZgnw6eDxo8DdZRx3zA15zyt029F0GjZvUuKdW5kZb3cL6+vJdnST/ca3qbv1O+7lYwlS51xE7/w3kV5wGBxzLCIwaxZ4A31www3wne+4ks6MGXDoofD44/DEE/DFL8JZZ8Exx7hg0dEBqRTcdZfb/vjj4bzz3OPwhwUumNTUuC9sPl/4Ik6Z4n68dXWjfjGzGSXiZ/Hi0RGBJfwNlh1vMpnCDmFGEI+7dKXThcyzqqp0mrLZQsaTSrll8bg75sCAWx/+sMNgWFvrjp3JFDLTgYHCD7i2tvA/DKrhexfuE/6oQ2HaVQuvWZzZhhlKJuM6IvT1wbRpQ9Mbi7ljZDJueSLh9slmCxlRIjE0swszMShcLKRS7pjxeCEjyuXcut5e97+/35VoOzpcu9b06W7brVvd96U4A83n3bKnnnL7lxJmaNFo4fNQda8D0NyMBt81mT7dnfvUqWgmgwwMoKkUUl/vPudwfXU16kXQDRvQ2nryc+fhpdNIdRKZPg1tmU6kqwN/fRv+QAZtmIIfiUHHdkSVSD4LKP6MWYjv4/f146cyeFs3I+vXIzvake5u/KlNaFMzkhpAoy7tflUt9PUi7duRrk7o7cWvb8RvaCQ1/1Ai2QyRvi6i+QxaXY0mqvBqktDVhWzeDB0dSH8fkupHEgm89m3Q34dWVaMNjfhTm9FojPyChfh1U9z73tdHvrrW1QTEE+ghhxKdNhVPFNE8EU9RHzzNE2lqwPurM8r8kZWvnBLE0cCPgQbcKOoO4EOq+swu9rsAOEtVLwuefwA4UVU/WbTNfwFPquqNInIecBfQrKrtIpIDVgI54HpVHRE8ROSjwEcBDjrooONef/31Mk97qHhcyWSDHGzrVlcNs5fC32pvt09PD2RyHr3rtpPs3caMhXVUaT969jlE1rwEgN/QSOe/3Uz6uFMGf+/hhWUuB83N7njZLFTHssS7txGb2UJGY2hPL97HP4rX9gaR118bPWEf+YgLFPE4OncevPgictyx6NQmMus3k0vWkW9sxs/5eLkMMTL4M2YzQBV+3wB+zifT1U8iKfiZHNFcmu5uJRpRojEhOq0JqakmNaCI+mS7+snlIR6DunqhKqkka6N4uYzLPLJZlwGGV1x9fYW0lmqLiUTcm5NMQkODy3AikULG2d/vflDptGvQnznTvU5PD6xf79p6wO3b0+Oq7aqrYd062LDBHXfTJtfGU10Nra0uHYceCgcdVPhgwwy5q8u9dnOzyyzDzHn7dvd848ZCptjfD6+84l47k3HbrF4Na9a49dOmue/e7NkuU/c8VzXY3e06IUyd6h6vW+deu7XVXST09RWuOFMp9x2OBtd+GzYMveiZNcu9T9u2lf9dDjP5SAS8COp5UFtH/i2nkj/qGLI1jagqms0h+RzxbB+8sd7d5z2XxfNzeBEPBLLVDeTSPmzZgvruvYy1byK6fQteZzt+PAmJJBpPIH09eKl+Iu1bkQrfL169CLkpTeRrGoh2bCXa14UfjSP5LIrgqU/ei9Jf00J/cir9WkVVqpOGgc1U5V2Q9BH8SJxoPj3k2L2JqXQlppFJ1KGxOHHJ0lczjXSynkh/H1UD7dT0byOaS9HU8/rgsdJeNQm/H4/R82iA9GFHEX9hBeLtfgljtBLELgNE0UHqAVS1u8ztywkQs4CbgPnAE8D5wBGq2ikis1V1g4gsAB4BzlDVV3b2entTxZRIKOlMECA2b3ZXLHtCFV5/HX/KVNq662mOd7Px+XZyvgcIjfFeMskG0hmh6dav0njLNwZ33frr5fjTZg49FkAmjaazDKQFQcknasj5HtGou7hKp5S8774Uvg9Vq1fQdMd3UBH8ZDW5Nx1D9pRT8afNYMr//09UPXL/6KcgQq55BqnDjyN1xFIGTjiN7NTpqAienyOaiOBVJdyFdsRD80q8tx1JDZCZMRcNMsOIB3hCJOohnuD7kBpQd/EsSjTuEY/kkR3tZHf04iWiVD3xG+qf/yNeYwMqAi+sQhYtdFfuf/mLy+BjMXjTm+Dv/95dEe/Y4U5840Z47DHXkL9wIfzud/D88y5jnTbNZfrFwSc839paEA/p2fXXOj99Jv70WfiNU/F6uok+vxLJpHe535DXq6mFgX6IRKGmBv/wN+EvPcFdXXZ34T3/DJLPo4kkdLSjU5vJ1zfChjYEoLaG/LyF5PHw1r9BZMsG/IZGiMXwJQrq48+egyKQzZFrnUe+pg5JDRDv2Ey0t5N8VR25qS34VbXkq2vJJWsRlOy02Wj9FLLNM4hvfB3yOXLTZpFvnlFIv68IhdKTaJ6+VIRsTqiK56lKuO9jZ4/H1Ck+uXzwXcDFr5deS/DSqwnatsbYuC1GTZXP3Fk5vKgwkPZIZTxEIJNSpjXlaZ2ZI9Wfp+fV7bRvTOPlMqzWQ/A2b6SmZwvNtSmSdVGaMxtpzGyhN9nEqtQCevNVxLu209sDnbFmamqgvjlOZiBPfd9metIxdqSq6M0m2Mgs1jEPn0LVXpw0GRLhWVNLL33UoMH0ddGIUlvtU5PMMy/axuudDWwYaETxiJKligHq6KGLBvqopSrpk84Ivj96Bt7Edmq9ASIzmqmq8fCzPslcN7meAWZ2raaWXtwnUPgDmHlwNf/x8l/tuwAhIn+nqj8VkU+XWq+qX9/Fi54MfElV3xk8vyrY77qdbF8LvKSqrSXW3Yprq7hzZ6+3NwEimVRS6eBbvHGju+rcHeHl/qZN0NVFXzrKGwMtVPVvJ5YdIBoX8nnwmpuIPfcXYk88TN1Pv0P2qOPJHnwo6be/m8wJb3HHymbBzyPpFJLP41dVQ3UNKoIA0t0Ffp58vIpczwCeJ67IWZVAwuqGomQNDBRqa2IRH558kvim18nOPZiaLa9Cewd1d3yfaPsWBs58D/mZrSQfe4DoG64k4tc10P7TXxNpe53E7x9GBgaIbN4AQKRtHdH16wZfz09W0Xfp5eRnzkayGWLPryD+lz/h1zcSXf8amkyi0RiSTuN1dqCJJF7/0CqKfN0UvN4uRJXU4ccR277RnciCBeisVhQh+sTDyPbtO/88IhH8Y4/Dr6kn3zIdv6MTqqpIH3UC/dPmQS6P19eNpAao+Z8HyTc2kV58DNmWWUg2TbZ1PrnZc5FMmvjq54ik+kmsfZ7qpx5HBvrwBvrJNU3Db5pGfsYsJJt1hYqqanJ1UxBV/OYWF8zrp5KdOadQ7dxUyGwl4qr5NJPBy+cgEnEFlETcXfQHJRXxc0RjApGou+r283j4riYwGkURslmlr1+oqcoTwaezS8irRyKap3MHdPZGWHxwBk+gL+WRScPrG6Os3xxjR5dHR1eEZAKiUZ+2zXGSCZ/qpDKjOQcCOd/juZcTPPVMEvFAETJZj45Oj96+wndORIlFIZMtLEvElaYmpbtbBrf1PGX6NGXHDiGVLmzreYrvCyKK6tDMrqbKJxZVGhvyzJudozqRp70rQmd3BAU6uyNUJ32aG/PU1vg01uWZ1pQjmxO274iwpSNGbbVPbdKtr6nyqa12Aa2uXsh7EWLxCLU1Si6VI50VmhvzzGzJU12lVFf5RKtjxBMRprUosYQHmTSSGgBVelJxNm2P0TwlRzSqZFKKiE99Y5RoTRIVj95UhI7tPnkiiOe5ToexPFHPJ0qeKFkiqT7Ui6LxuPs+qA+5PDu259jRoWSywvNrEmSyHr4PWzpiVFcr//LNqfs0QHxMVb8nIl8ssVpV9ZpdvGgU10h9BrAB10j9PlV9oWibZqBDVX0RuRbIq+oXRKQR6FfVdLDNH4Fzihu4h9ubAFFVpQykggDx6qswb175jUGdna4LqudBJoNW17D+DYWBfiQRJ1aTGNw0/qcnmHr5JQBkFy2m6ys3kVt4mFuZz7ur0XwOojH8qc2QzaJTGkc0Wkr7dveljETwm6chA/1I1w683h40FkO9oEpAxAWbbBZVV+WTi1URScSGHq+zw2U+dfWFZd2dRNe+xNRPXIJkM0O2zy5cDBEPv34KvQuPZgszOMhbT+1/3jxkO7+qhh3TDiGS6sNrnEIyCZpIkJ82E01WEdm8gczxb8ZvaCT66hrSbz6dzLEnk+rNEa+KIJ4M1j5ls4Xjeql+mp57DK++lkh1nEhPF9nDjiA3pYVMVsj1Z+iXGsjnXbtpwmW8Xi5N1PMRVYh4iO+76pJEErKuLUFUUQTx87jhk0LO9/jLqiQ5L46KR9NUn+3tHlu2u7r8nh7htY1xGuvzZLPC9h0etVU+h81PU5Xw2bgtSmN9nrmzsqgPA2l3lVxT5VNdpSQbkyTr49RE09RV5d3nGY3w/EsR/vhMDc+uSbKuLcpB0zMokPUjrFobZ8Nm14hcU630DQiptIeIBs0bI7+/yYSSTCid3SMbhMKMGaCuTslmXS1dcSbd0ADHHusKbrGY+9/Q4Gq6EglXu9XT42rWWloKTUZ9fe66a+pUt//RR7ufWNgUkkoFVafV7jjgrgl27HC1hNGo66yXTFKo4oNCG0/4vLjRN2hj07yPijtfL1K0H5D3hbwvQzpeDe+UFb5E2DQVpm3I70eGJiNUPAnCzhTXWJY6TnFz12CwCFZ4QaAO38dDDtmzNuy9qmISkTer6u93tWwn+74buAHXzfVHqnqtiFwDLFfVe4NqqOtwv8QngMuDoHAK8D3Ax01JfoOq/nC019qbAFFdrfQPBD+aF1+ERYvKH3T2yiuFevTm5sGq7qlTh24WeW0tzZeciWQzDJx1Lj1X/H/4LcHVpCrS240mkq6aaVhJoCyqyICr3pFtW1ydre+jiQRaVYNW1yBZ1yAHYU+LsKFa0WS1CyaZDBov9CiJL/8DiZ98n+db3s7m9BTeaDiSFfkjqav12d7hsfKFOKvWxKhK+hy5KEXj9jXMSK/jjx2H8jKHDBbJAZYemWHenBxvOzFNbbVSX+vz3Esx1m+KIMBJR6d4z5s73A/bTRocvA3usfuFeKjvk85G8KMxNO3qiEUUz8/jeYpEPOIxCp9h8Cvv8Wt4bWs1W3fEePUVZWtHlNWr4YXVMXxcBjtzuk99nU8k6rFhk0dfP/T2CanU6J9HPKaDV821NT59/TLiCrgc8bjrL9DVVciokklYsgQ2bFCSSZehHXaYG7Lj+65pI5l0zQsbN7qMtqXFZWq+776LqvDssy7TnjvX1drNmeMy6qlT3T5hG37YVj4w4DLpsHmopWXnP4viDkGjdUjw/UKbfXFGHPaZKM6OinvL7mySg+IMtVSGHHYMCtvXiztyRaOF/g/FnZKK94lEhnZUKk5v+DwaLQSWMJ3h/sXbFqdj+Kw5xcEhPOfi9zoMUmEfh7DJzfMKfRb2tOl0bwPEX1T12F0tG297EyBqapS+/uDTWr4cjjqq0Mi3M77vGhk7Olw9d9CAGrZNJpMQaXud6p/fQvzpP7lqmdQAHT+6m9yhRwweQ7o7XVVSwxT81jEapFd8yTH815oOisTR6GDPFMllWfV0P/c+PoUnVtYR1RwvvRpjwZwMO7oivLFp5LiJSERprPfJ+zCzOUcqLcRiStNUd9W9aH6OulqftxzdS08PrFob577H6ljz+shjxWP+YCeB9/51hogHv3k0xhFL8hx6KMxoznPOe3zufyjOccfB0uOUSG8XMtCPVlUXzjeRQOMJ2jdnueeBOM+/FOX1dbA2aLnavn1ohi0Cra3Km94E1dXC2rWuw9aWLe4jbW11mXUy6a56m5shm8qzPmj7rq8XfDxaW93z3l6Xqc6e7a4X3njDXR03N7tC5iuvuKRWVbnjDwy4v7ATUX+/266312UOBx/shrzMmVOR3o2Dwgwm7AMQ9q6EQmYVdo4qXlecdYSdloqvwIcLe5hWVRWG/nieC1q5XKFXa7h/2Ichl3Pbhpl0mLmHQaW4IBGeQ5i5FmeyYdqLM3qz51VMJwOnAFcA3yhaVQ+cq6pHjXVC98beBIjaWqW3L/jW/OlPcNxxhQARhu1EYuhOqZTrfVJdDVVV7ov60/9Cv/512m/4CZGI0HzR0G5nXV0Bdh4AACAASURBVJ+/loELPzT4XPp63JV7NoM/Zx7qRXjwQffjOemkQvfoXUmnXU/X+fPh2992V4r5PPzjP8LFFxd+OKqFuHDjjXD//a49d8ECuOmmQp1vMgkzZyrplDKlzqexJk1zo8+ZJ3Vz0uE9TJsTxwMiVXE0Hgcv4t6fgX68rs7CpZMIWl2DxmKutOJ59KRjtG2MsGFLFD/nc+RR0NgSo6tb+Oa3hJ/9XOjrcxl1NlvoqVpszhw45xzXHl1T4zKP1193HXNeecUND8lk3Pt42GHuT8RdOR90kBvmsnixe41YbOTx90dhFdvwzDp8q2Fkxlyq6mL4FXfYUzYcKpNKua90cS/e8LixmHvPwu9kmJbiq+viIQjhX5iB+35hqIfZf+xpgDgVOA34R+C7Rat6gF+p6poxTude2ZsAUVer9IQB4rHH4JRTCjlHR4e7pDvooKE79fW5LoRBH/men91P3ac/CrjGWi81dKhI30V/T8/nvlxYkEkjuRz5uQsGA9A998AnPjH0Zc47zxVoLrvMJeVPf4LTT3c/5jVr4Kab4L77Cl3ld2XhQtdRa3j39WOOga9/3V21DglKvo/saHe/7ho3mE6ra3b+Kw/77YcDfnYzN+jocD05581zh8nlXLPQ44/DkUe6c777bnj66ZH71ta66pJTT4V3v9uNLxyr8X5h9//hy8KfT/GYplKZcrhuZ8J1YVVE8bgqGDncIZEYOWdj8fiv4qqK4mOGV9jhFb8NeDZ7W8U0V1Vfr0jKxtDeBIj6Wp/uviBXfPBBlwOHAWLrVtfydvDBQ3fascNVMW3YgP+tm/B+eRcAPX/3caqX/YJIx3a6/s/1DJz7/qG/4mwGr78XjSXIz2yFqiruvhu+/GWXcYdXZMWNsgCnnebGwu2s+H7yya564ktfckmtqXFB5bXX3Mufcgq88IIb+JpMwr/+qxt83dHh2tkPPvjAyijyeVcl09fn/oeT3O7O/uEA1+IBsMVX5FDIcKPRoJGUwhVxWO0Bbl1YHxyW/Iqv8CORoUG8+CtRPEg5k3HHqKpyy8MSkg1sN5WytyOpfyAif6uqncHBGoE7wu6rE0FUii4NU6mhlasDA4VL2TA36Opyuepvfwuf+MRgM+y2XzxKfsEh9P3jZxA/7/q8F1NFBgbIt8xAq2shmWTlSvjnfy5kHv/6r/De97rBzx0d8M53ulLFH/8IF13k6seXL3dX2UccAR/7mKsyOfXUked1111Dn/f0uELPYYcVljU3u7+90dtbyASLZy4onnWguPEuzEDD5dlsISOtri7UQ4d1z6VEIq66o65u5+kKM//iYBumKRz71dDgMvfi2Q3CQc5h1clYXWnvTgAzZn9QToBoDoMDgKruEJFpFUzTPjckQKSDgU++7wJBOKI1nS4EiP5+l9v+z/8Udlt6CvkFh7gnVVUlxz5Kfx9+UzM61eXIjz0G73+/Gwz7m9+4jKqmxm374Q8X9vvNb1wmN1pmWI66uqHBYXeEV7bDe5qExy3utRFOxxPWMsXjbl067TLe3t5ClU0i4QJcWK++bdvQapBwHEfx1X0YSEpdVYeNmrmce+1k0h0/zOjDmTLsityYXSsnQPgicpCqvgGuygnKGPt9AIlJrvAkLEFkMq7OJpt1uUwq5XLvYI4UXnwRbr+d/mPfTNc1NyJTppQ+uCrS18vvlse5/LoF3PSdCCef4hqTrwuGDP74x6PPD5hMFqo3Kq04gw2FGXl1tavnD9vrwy6B5VxZRyKFyWVHC3QNDS4ohFU2YXNGWEpJJgu9fcKAUTwNUZjOxkarmjFmb5UTIK4Gficij+M6pL+VYP6jiaJkCSKfZ7A7TTTqShMNDe5/V5frHgTkTzgZKR55XdzBOZPG6+vl1S01XPCZ+QD87YWFnkQA3/2uqyoab+m0i4GJROGUw0AQXrHvC55XKEVBIQ2NjYVlyaSbRzAMDGE/9j1oEzfGjKKcO8o9ICLHAicFi65Q1VHmOTjwRBgWIMJWw6qqwqV7KuXaHYo6g/edfAb97/uHwr6qSI+bJsKvqUP6+2hLNXPep93cTu95D/zqV+4Q/+t/wWc/u+vhFnsiHIwTNqSWksm4K/GwyiYedz2HDpR68uKSQSXeQ2NMeSUIgDywFUgCS0QEVX2icsnatz6Y+UHhSdjxPpcb2dWkq8tdSj//PABdF/4DXtgQHQx604ZG/KpqvPZttOls3nx+42Ch5Lvfhauvdo3PR43xKJJwNuZ0unAVHYm4mBaOg4jHC1P9x+Nu/EPY28aqY4wxw5Vzy9HLgE8Brbjpt0/CzY309sombd/50sDnC09SKXd53d3tcs7Pfc7NHDp9ustd6+tdVyPAn32Q68GUTiHpFDq12c2hFI2Sb5jCly93k/TV17vbOIAb5DVnztilPZstTF0QibjqmZkzC4WgrVtdlZGqaz9oby9UI4XjHQ6UwWLGmH2rnBLEp4DjgT+p6ukichjwr5VN1jgaGHAtor29boTWf/6nG5p7//3g+/gP/AZv5UoA/JmzIDXg5j2KxvCb3GQ1jz8Ov/2tcM89cMUV8L//99glL2xDD6cmCK/+p01zwWF4D53WYXPj7ulM5saYyaecAJFS1ZSIICIJVX1JRA6teMrGy803uxFmmYybShLc46Ayv2flKzTgbtARScSQgRT+jFmDM3vl8/C+97ndpkwZOTJ6V3I5F5tiMRerwptx1da64BA2xIZNI9Onu3XWOGuMGWvlBIg2EZmCux3oQyKyA9jvR1bvsW3bXOttTY0bhgxugp+BAXTFShq+cx0qwmvfXsYUz/Wt1JpCDv2HPxQOdfXVQ3vklGNgwJUGursLo2jr6lxVkZsgzoKBMWbfKKcX07nBwy+JyKO4W48+UNFUjbewxBDeClIVXngB/d7NCDBw7vuYUu/jdXXi1xbu16wKl1/uru6ff373egRls65XbTzuRjaH3TrDdoKGBgsMxph9a7c6CKrq45VKyH6lpcW1Odx9t5v2c/VquOUW5OGH6D39PfReeR3ksvg1tfjNhUr95ctdI/Bb37r7wSGddkGhPrhnz/BZXC04GGP2NetBPlw4IvpjH3P/Dz7YzRN9990I0H3upXjBzGvaMGXIEOeXXnL/v/KV8l/O913JYe7c3a+OMsaYSrKe78DL0cUA+Ge+07USb9lSWPmVr7ibMwB+bT25Y08AcLekHDYK7fbb3bxKCxaU/9o9Pa7UYMHBGLO/sQABePj8Yfp70dmzXX1P2Hvpu9911U1BqSKz4FAi8QjkckguM1gPtHWrm2n1mWfg4x8vf8BZb6+LMbNnV+KsjDFm7+y0iklEeig9KZ8Aqqr1JdYdkKq1j5RXhUaDocbd3W5FOMdScC+IznM/7GYZ3dGFxmJoxL19d90Fv/udayc4++ydv044d1A4S2lVleumau0Lxpj90U4DhKru5eTSB45qv4+0VwOxqBt0EAaIhgbXSHDEEfT96Vl6003UggsOU6ZCLMbmzfD977uM/le/KtzwvZSursLEctXVLv7YPELGmP1V2VVMIjJNRA4K/8rc5ywRWS0ia0XkyhLr54rIwyLyrIg8JiKtw9bXi0ibiNxUbjr3RFL7SUWq0Ugw58QTwTRTdXUuWHR10RtvclNSBNOw+lObQYTvfMf1XLrlltGrivr63BQX4ViG1lYLDsaY/dsuA4SInC0ia4DXgMeBdcCvy9gvAnwbeBewBLhERJYM2+yrwG2qeiRwDXDdsPVfBio7KWA+T5I0A1KNHw0CxM9+5v4HfU79SIz+/uD+B/29aI27Q05nJ/zXf7k7wO1q8j3fd8Ghudnd3npns6waY8z+opwSxJdxE/S9rKrzgTOAP5Wx3wnAWlV9VVUzwB3AOcO2WQI8Ejx+tHi9iBwHTAd+U8Zr7bm+PgBSkRry3tBL+rRXBZEIvfkkA51pPD+HxuL4M2bR2Qmf+pRrS/iHfyh14IJwWu19ddMfY4wZC+UEiKyqtgOeiHiq+ihQ8gbXw8wG1hc9bwuWFXsGOC94fC5QJyJNIuIBXwM+O9oLiMhHRWS5iCzftm1bGUkqob8fcMFAo4VpTXPfv4Xt7UImK6SrGqmJZ5BsBn/KVBDh9tvdLak/9anRb/gT3g85vC2nMcYcKMrJsjpFpBZX1fOfInIj0DdGr/9Z4FQRWQGcCmzA3XviE8AyVW0bbWdVvVlVl6rq0paWlj1LwYwZLJyX5YEZH4J8odNWT3wqA/1KJiu0Z+qIJzwkkxosBvzhD268w+c+N/rhBwZc20TxTeeMMeZAUE4z6TlACvhn4P24uZiuKWO/DUDxnQ9ag2WDVHUjQQkiCELnq2qniJwMvFVEPgHUAnER6VXVEQ3dY8Lz8IlAe+FGedI4hWwqR3csSqw+CvV1aI+iySryefjzn0fv0gquPTseP3Du0maMMcXKmayvD1yPIuBXu3HsPwOLRGQ+LjBcDLyveAMRaQY6VNUHrgJ+FLzm+4u2+RCwtGLBgcKN773OjsFlvdFGvK5Ock0z8bxg8Ec0CiKsWuVGQJ9yyujHTaXs/gvGmANXOb2YPiYim4FngeXA08H/UalqDvgk8CDwIvBzVX1BRK4RkfDa+zRgtYi8jGuQvnaPzmIvibgRgbnZrvdu/qqrSUVqyFXV0h+fgucFwSHpigJr17r9Dj9858fs73dBx6bQMMYcqMqpYvoscISqbt/llsOo6jJg2bBlXyh6fCdw5y6OcStw6+6+9u4ISxC9l3yUurccQ/a4NyOvpmlobWC7H6c2AtrYNDisfPNm93/GjNLHy2Zdt9baWhvrYIw5cJWTfb0C9Fc6IeNJBFDwvSgcdxwd7Uou41Ndk6SpKux9VJgPY8MGVzKo28lY83R6yBROxhhzQConQFwF/EFEngTS4UJV/aeKpWofC6uYNBIFL0c+HwyYFhlyX4aeHrj1Vjdq+phjSh9LtVB6sDmWjDEHsnICxPdwg9meA/zKJmd8hFVMGo1D3Cca9alvYMTAhZ/+FK6/3j2+7LLSx+rvd72WYrHS640x5kBRToCIqeqnK56SceSu9AU/CH++H5YqhhYBwhsCPfooHHJI6WOpuuolY4w50JUzUO7XwYjlmSIyNfyreMr2ocEqJgU8D/UVL5sZUUf08svwtreNHhxUrfRgjJkYyilBXBL8v6pomQK7cd+0/Zu7wQVINgOAZnJQPbSeyPdhzRp43/t2chBc43RdnQUIY8zEUM5Aufn7IiHjabCKKeeDF0WzWXRqM1LUQr1hg5s249BDSx8jlXLtDzYwzhgzUYx2R7m3q+ojInJeqfWq+svKJWvfCtsb/GCgg+byyLABDC+/7P7vrHopm4U5c9yNgIwxZiIYrQRxKq730ntKrFNgggUIR8VD84rEokPut7pmjfu/cOHI/TMZV61UX29dW40xE8dotxz9YvDwGlV9rXhdML/ShDHYY0k8Nz13eF/QIqtXu9uJNjaO3D+VgnnzLDgYYyaWcnox3VVi2ajTYxxownEQ+UQ13dvSwYC5oW/NmjWlq5fCGVvtZkDGmIlmtDaIw4DDgYZh7RD1wITKDgcDRHUdvTt68Gtqh4yByGRcCeKSS0buazO2GmMmqtHaIA4F/gaYwtB2iB5gFzfZPLCEVUx5X8iroLHEkJs4PPOM66F08slD98tm3b42Y6sxZiIarQ3iHuAeETlZVf+4D9O0zw1OtaHg+4IOq14KR1C/6U1D90ul3N3ibMZWY8xEVE4bxBYR+ZWIbBORrSJyj4hMmEFyUAgQvgrZmilkqxuGrF+71rUxzJpVWKbqmims9GCMmajKCRD/BfwcmAnMAn4B3F7JRO1rxXMxZeK1aGJoE8vatXDwwUPbrbNZFzSs55IxZqIqJ0BUq+pPVDUX/P2UCdZIHYkovg/NLUJ9/dD7PGzfDr//PSxePHSfXM4GxRljJrZyas9/LSJXAnfgxpNdBCwLJ+xT1Y7Rdj4QRKOQGXD3fogMe0eefdaVFs4/f+hy34dEYt+l0Rhj9rVyAsSFwf+PDVt+MRNk0r5YFPqCGwQNF06xsWTJ0OWqDLmZkDHGTDQ2WR+uBJH3RwYHVfjVr9z0Gs3NQ9eJWIAwxkxso7ZBiMg0EfkXEbkz+PsXEZlW7sFF5CwRWS0ia4NqquHr54rIwyLyrIg8JiKtRcv/IiIrReQFEfnH3T+18kWjkM0NDRD33gsXXwwrV8KHPjRyHytBGGMmup0GCBF5M/Dn4OltwR/AU8G6UYlIBPg28C5gCXCJiAyrqOGrwG2qeiRwDXBdsHwTcLKqHg2cCFwpIrOokFhMyQ0LEB//OPzud+7xGWcM3T6Vcu0PFiCMMRPZaFVMXwPeq6oripbdKyL/jbtP9Ym7OPYJwFpVfRVARO4AzgFWFW2zBAhvZ/oocDeAqmaKtklQXm+rPRaLQr7obtvZ7ND104rKTL7vbgw0b14lU2SMMeNvtIy3flhwAEBVVwJ1JbYfbjawvuh5W7Cs2DNAOM/TuUCdiDQBiMgcEXk2OMb/VdWNw18guBXqchFZvm3btjKSVFo0NrSKaeXKoeuLJ+Lr7XUBo2gmDmOMmZBGCxAiIiMmtw66t47VFf1ngVNFZAXu/hMbgDyAqq4Pqp4WApeKyIgp8VT1ZlVdqqpLW1pa9jgRsaiSzxcCxGixxvNKT/ltjDETzWgZ/TeA34jIqSJSF/ydBvw6WLcrG4A5Rc9bg2WDVHWjqp6nqscAVwfLOodvAzwPvLWM19wj0agb+BZqby88PuywwuN83t0YyKtohZcxxuwfRpus72YR2Qh8GTftt+LaD76iqr8q49h/BhYFNxfagBs38b7iDUSkGehQVR+4CvhRsLwVaFfVgaAU8xbKC0p7JBaFbH5kFdNLLw0dVZ3Pu3s/GGPMZDDqOAhVvQ+4b08OrKo5Efkk8CAQAX6kqi+IyDXAclW9FzgNuE5EFHgCuDzYfTHwtWC5AF9V1ef2JB3liEaVfL7w/Mkn4cQThwYHcAGitrZSqTDGmP1LRSeqVtVlwLJhy75Q9PhOStydTlUfAo6sZNqKxWIM6eba2QlvLVGhZSUIY8xkYrXpuEbqsBeT70NXFzQ0lN7W7v1gjJksLEAQTLURVDEtX+6CxM7uMW2D44wxk0XZAUJEThKRB4IpMd5byUTta7FgLiZVuOcet2zKlJHb2fQaxpjJZKcVJiIyQ1U3Fy36NG4wmwBPEox6nghiUQXcCOlw1PRFF43czvMsQBhjJo/RatS/KyJ/Af5NVVNAJ3AB4APd+yJx+8qig908G6tWuSAhMrKKKZ93VVF2BzljzGSx0yomVX0vsAK4T0Q+CFyBmxepCZhQVUxHLHEB4o033ER8pW4lms3a9BrGmMll1DaIYEDcO4EG4L+Bl1X1m6q65xMf7YeSCRcg0unCTK3D5XIWIIwxk8to032fLSKPAg/gprq4CDhHRO4QkYP3VQL3hWQQEDIZFyRK9WBSdeMljDFmshitDeIruCm7q4AHVfUE4DMisgi4Fjd1xoSQTBYaqcMqpuFEbAyEMWZyGS3L68JNxV0NbA0XquoaJlBwAEjEdh0gwHowGWMml9HaIM7FNUhHGTbJ3kSTKKOKScRmcTXGTC6jzea6HfjWPkzLuIlEIBpRUimhs3PkJH25nHVxNcZMPnZNDKBKIq5kMrBpE8yYMXR1JgP19eOTNGOMGS8WIALxuNLf7+4mNzxA5POlu74aY8xEZgECIJkkHoeODhcMht9S1HowGWMmI8v2AKmuIlHl7gMBUF09dL2NgTDGTEZWggjE44UAUTxiOpNxAcO6uBpjJhsLEIF43E3WByMDhDVQG2MmIwsQgY0bC4+Hz7lk1UvGmMnIAkQgrF6CQoDo7HR3l7PqJWPMZFTRACEiZ4nIahFZKyJXllg/V0QeFpFngzvVtQbLjxaRP4rIC8G6ErfvqZx43P3PZt0gORsgZ4yZjCoWIEQkAnwbeBewBLhERJYM2+yrwG2qeiRwDXBdsLwf+KCqHg6cBdwgIiVuAloZIq7nUjTqptewKTaMMZNRJbO+E4C1qvqqqmaAO4Bzhm2zBHgkePxouF5VXw4mBURVN+ImC2ypYFoHXXstHHOMKznEYq4HkwUIY8xkVMmsbzawvuh5W7Cs2DO4GWPBTQ5YJyJNxRuIyAlAHHhl+AuIyEdFZLmILN+2bWzuYXTRRa4Ekcm4tohk0gKEMWZyGu+s77PAqSKyAjgV2ADkw5UiMhP4CfBhVfWH76yqN6vqUlVd2tIyNgWMcCZXVde9ddq0MTmsMcYccCoZIDYAc4qetwbLBqnqRlU9T1WPAa4OlnUCiEg9cD9wtar+qYLpBOCww9z/4gbpWMxKD8aYyauSU238GVgkIvNxgeFiht1XQkSagY6gdHAV8KNgeRx3D+zbVPXOCqYxSAf89KcwMDB0uQUHY8xkVrEsUFVzwCeBB4EXgZ+r6gsico2InB1sdhqwWkReBqbjbmUKcCHwNuBDIrIy+Du6UmkFdw+IBQuK028BwhgzuYmqjncaxsTSpUt1+fLle7SvKqxZA7W17rnvu1uPHnywjYEwxkxsIvK0qi4ttc6ukUsIR09bcDDGTGYWIEoIB8kZY8xkZgGiBN+3AGGMMRYgSrAGamOMsQABjGxryOUKE/YZY8xkZQFiJ4bfE8IYYyYbCxABz3NVSyHrwWSMmewsQOyEBQhjzGRnASIwvARhjDGTnQWIElStBGGMMRYgAuFd5MLHFiCMMZOdBYhAcYAInxtjzGRmASIwPCBYgDDGTHYWIALFJQhrrDbGGAsQg4pLDNYGYYwxFiAGhd1cw3mYLEAYYyY7CxDDhPeCMMaYyc4mtQ6IuEn6RGyqb2OMAStBDKquhnze7gVhjDEhCxCBcHpvuxeEMcY4Fc0KReQsEVktImtF5MoS6+eKyMMi8qyIPCYirUXrHhCRThG5r5JpLLye+28BwhhjnIplhSISAb4NvAtYAlwiIkuGbfZV4DZVPRK4BriuaN2/Ax+oVPqGK+7aagHCGGMqW4I4AVirqq+qaga4Azhn2DZLgEeCx48Wr1fVh4GeCqZviOIShDHGmMoGiNnA+qLnbcGyYs8A5wWPzwXqRKSp3BcQkY+KyHIRWb5t27a9SmzxuAcrQRhjzPg3Un8WOFVEVgCnAhuAfLk7q+rNqrpUVZe2tLTsVUKsDcIYY4aqZIfODcCcouetwbJBqrqRoAQhIrXA+araWcE07VQ4DiKdhhkzxiMFxhizf6nktfKfgUUiMl9E4sDFwL3FG4hIs4iEabgK+FEF0zOqsJE6FrNpNowxBioYIFQ1B3wSeBB4Efi5qr4gIteIyNnBZqcBq0XkZWA6cG24v4j8D/AL4AwRaRORd1Yqre713F9VlStJGGPMZFfRMcOqugxYNmzZF4oe3wncuZN931rJtA1XHCASiX35ysYYs3+y5tgingf19S5IGGPMZGcBIiBi03wbY0wxCxDGGGNKsgBRxMY/GGNMgWWJRayKyRhjCixAFLEShDHGFFiWWMRKD8YYU2ABokgk4kZSG2OMsXtSDzFz5ninwBhj9h9WgjDGGFOSBQhjjDElWYAwxhhTkgUIY4wxJVmAMMYYU5IFCGOMMSVZgDDGGFOSBQhjjDEliaqOdxrGhIhsA17fi0M0A9vHKDkHCjvniW+ynS/YOe+uuaraUmrFhAkQe0tElqvq0vFOx75k5zzxTbbzBTvnsWRVTMYYY0qyAGGMMaYkCxAFN493AsaBnfPEN9nOF+ycx4y1QRhjjCnJShDGGGNKsgBhjDGmpEkfIETkLBFZLSJrReTK8U7PWBGROSLyqIisEpEXRORTwfKpIvKQiKwJ/jcGy0VEvhm8D8+KyLHjewZ7TkQiIrJCRO4Lns8XkSeDc/uZiMSD5Yng+dpg/bzxTPeeEpEpInKniLwkIi+KyMkT/XMWkX8OvtfPi8jtIpKcaJ+ziPxIRLaKyPNFy3b7cxWRS4Pt14jIpbuThkkdIEQkAnwbeBewBLhERJaMb6rGTA74jKouAU4CLg/O7UrgYVVdBDwcPAf3HiwK/j4K/Me+T/KY+RTwYtHz/wt8Q1UXAjuAjwTLPwLsCJZ/I9juQHQj8ICqHgYchTv3Cfs5i8hs4J+Apap6BBABLmbifc63AmcNW7Zbn6uITAW+CJwInAB8MQwqZVHVSfsHnAw8WPT8KuCq8U5Xhc71HuBMYDUwM1g2E1gdPP4ecEnR9oPbHUh/QGvww3k7cB8guBGm0eGfOfAgcHLwOBpsJ+N9Drt5vg3Aa8PTPZE/Z2A2sB6YGnxu9wHvnIifMzAPeH5PP1fgEuB7RcuHbLerv0ldgqDwRQu1BcsmlKBIfQzwJDBdVTcFqzYD04PHE+W9uAH4HOAHz5uATlXNBc+Lz2vwnIP1XcH2B5L5wDbglqBa7QciUsME/pxVdQPwVeANYBPuc3uaif05h3b3c92rz3uyB4gJT0RqgbuAK1S1u3idukuKCdPPWUT+Btiqqk+Pd1r2oShwLPAfqnoM0Eeh2gGYkJ9zI3AOLjjOAmoYWRUz4e2Lz3WyB4gNwJyi563BsglBRGK44PCfqvrLYPEWEZkZrJ8JbA2WT4T34s3A2SKyDrgDV810IzBFRKLBNsXnNXjOwfoGoH1fJngMtAFtqvpk8PxOXMCYyJ/zXwGvqeo2Vc0Cv8R99hP5cw7t7ue6V5/3ZA8QfwYWBb0f4riGrnvHOU1jQkQE+CHwoqp+vWjVvUDYk+FSXNtEuPyDQW+Ik4CuoqLsAUFVr1LVVlWdh/ssH1HV9wOPAhcEmw0/5/C9uCDY/oC60lbVzcB6ETk0WHQGsIoJ/DnjqpZOEpHq4HsenvOE/ZyL7O7n+iDwDhFpDEpe7wiWlWe8G2HG+w94N/Ay8Apw9XinZwzP6y244uezwMrg7924uteHgTXAb4GpwfaC69H1CvAcrofIuJ/HXpz/acB9weMFwFPAWuAXQCJYjj3FbwAAAnRJREFUngyerw3WLxjvdO/huR4NLA8+67uBxon+OQP/ArwEPA/8BEhMtM8ZuB3XxpLFlRQ/siefK/D3wbmvBT68O2mwqTaMMcaUNNmrmIwxxuyEBQhjjDElWYAwxhhTkgUIY4wxJVmAMMYYU5IFCGP2gIhcHcwm+qyIrBSRE0XkChGpHu+0GTNWrJurMbtJRE4Gvg6cpqppEWkG4sAfcP3Pt49rAo0ZI1aCMGb3zQS2q2oaIAgIF+DmBXpURB4FEJF3iMgfReQvIvKLYF4sRGSdiPybiDwnIk+JyMJg+d8G9zd4RkSeGJ9TM6bAShDG7KYgo/8dUI0bzfozVX08mANqqapuD0oVvwTepap9IvJ53Mjea4Ltvq+q14rIB4ELVfVvROQ54CxV3SAiU1S1c1xO0JiAlSCM2U2q2gsch7sxyzb+X3t3jEtBFMVh/PsnEgUSVqAQtBIFolJbgLxOI7EIsQGlDejEDiSa17ygkKDWaGxApZCjmCkmL4NonpDv19zJnXsnM9WZc29yLlwk2R8btklzCNUoyT1N3ZzFzv3zTrvVXo+AsyQHNIfgSL9q6vshksZV1TswBIbtn//4UY4Brqpq8Nkjxq+r6jDJBrAL3CVZr6q/WnVU/4AZhPRDSVaTLHe61oBn4BWYa/tugO3O/sJMkpXOnL1Oe92OWaqq26o6pslMumWapYkzg5B+bhY4TTJPc/b3E81y0wC4TPJSVTvtstN5kul23hFN5WCAhSSPwFs7D+CkDTyhqdj5MJGvkT7hJrU0Yd3N7N9+F+krLjFJknqZQUiSeplBSJJ6GSAkSb0MEJKkXgYISVIvA4QkqdcHAryCfHBiCvoAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Social Learning"
      ],
      "metadata": {
        "id": "BX5FnIHPghU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ucXZ7Lq3RMO"
      },
      "outputs": [],
      "source": [
        "class Sciety():\n",
        "    def __init__(self, env=None, thompson_agent = 0, percent_agents = 1, p = [0.01], increase = [0.005], limit = [0.9], worst_agents = 1, random_agents = 4, UCBagent = 0):\n",
        "        self.agents = []\n",
        "        self.names = []\n",
        "        self.thompson_agent, self.percent_agents , self.p , self.increase , self.limit, self.worst_agents , self.random_agents, self.env = thompson_agent , percent_agents , p , increase , limit , worst_agents , random_agents, env\n",
        "        self.historyTotal = {}\n",
        "        self.UCBagent = UCBagent\n",
        "        for i in range(thompson_agent+percent_agents+worst_agents+random_agents +UCBagent):\n",
        "            self.historyTotal[i] = []\n",
        "        self.historyTotalR = {}\n",
        "        for i in range(thompson_agent+percent_agents+worst_agents+random_agents +UCBagent):\n",
        "            self.historyTotalR[i] = []\n",
        "        \n",
        "    def reset(self, env):\n",
        "        self.env = env\n",
        "        self.agents = []\n",
        "        self.names = []\n",
        "        for i in range(self.UCBagent):\n",
        "            self.agents.append(UCB(environment = self.env,id = len(self.agents)))\n",
        "        for i in range(self.thompson_agent):\n",
        "            self.agents.append(ThompsonAgent(environment = self.env,id = len(self.agents)))\n",
        "            self.names.append(\"thompson\")\n",
        "        for i in range(self.random_agents):\n",
        "            self.agents.append(alwaysRandomAgent(self.env,len(self.agents)))\n",
        "            self.names.append(\"random\")\n",
        "        for i in range(self.percent_agents):\n",
        "            self.agents.append(percentBestAgent(self.env,0.1,[1,1,1,1],len(self.agents), self.p[i], self.increase[i],self.limit[i]))\n",
        "            self.names.append(\"percent(\"+str(self.p[i])+\",\"+str(self.increase[i])+\",\"+str(self.limit[i])+\")\")\n",
        "        for i in range(self.worst_agents):\n",
        "            self.agents.append(alwaysWorstAgent(self.env,len(self.agents)))\n",
        "            self.names.append(\"worst\")\n",
        "        self.history = {}\n",
        "        for i in range(len(self.agents)):\n",
        "            self.history[i] = []\n",
        "        self.historyR = {}\n",
        "        for i in range(len(self.agents)):\n",
        "            self.historyR[i] = []\n",
        "        \n",
        "    def step(self):\n",
        "        for a in range(len(self.agents)):\n",
        "            obs, Ri, index_selected_arm = self.agents[a].take_action()\n",
        "            self.history[a].append(index_selected_arm)\n",
        "            self.historyR[a].append(Ri)\n",
        "    def historyRecord(self):\n",
        "        for a in range(len(self.agents)):\n",
        "            self.historyTotal[a].append(self.history[a])\n",
        "            self.historyTotalR[a].append(self.historyR[a])\n",
        "        self.history = {}\n",
        "        for i in range(len(self.agents)):\n",
        "            self.history[i] = []\n",
        "        self.historyR = {}\n",
        "        for i in range(len(self.agents)):\n",
        "            self.historyR[i] = []\n",
        "    def getCategories(self):\n",
        "        temp = {}\n",
        "        index = []\n",
        "        for n in range(len(self.names)):\n",
        "            if self.names[n] not in temp.keys() or \"percent\" in self.names[n]:\n",
        "                temp[self.names[n]] = 0\n",
        "                index.append(n)\n",
        "        return index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvTgi4QH14tS"
      },
      "outputs": [],
      "source": [
        "class SocialLearning:\n",
        "    def __init__(self, learner, society, individual, k_n_thr = 0, limit = 10000):\n",
        "        self.learner = learner\n",
        "        self.society = society\n",
        "        self.individual = individual\n",
        "        self.history = []\n",
        "        self.historyR = []\n",
        "        self.historyindividual = []\n",
        "        self.historyindividualR = []\n",
        "        self.best = []\n",
        "        self.bestR = []\n",
        "        self.preference_history = {}\n",
        "        self.id = 0\n",
        "        self.k_n_thr = k_n_thr\n",
        "        self.limit = limit\n",
        "\n",
        "    def iteration(self, env, trial_cnt, preferencelearner, threshold = 1000, noize = 0):\n",
        "        self.society.reset(env)\n",
        "        history = []\n",
        "        historyR = []\n",
        "        historyindividual = []\n",
        "        historyindividualR = []\n",
        "        if env.arm_count/len(self.society.agents) > self.k_n_thr:\n",
        "            socialLearner = self.learner(env,0.1,len(self.society.agents), preferencelearner, noize, self.limit)\n",
        "            self.id = socialLearner.id\n",
        "        else:\n",
        "            socialLearner = self.individual(environment = env,id = 0)\n",
        "        for i in range(trial_cnt):\n",
        "            self.society.step()\n",
        "            obs, Ri, index_selected_arm = socialLearner.take_action()\n",
        "            history.append(index_selected_arm)\n",
        "            historyR.append(Ri)\n",
        "        if env.arm_count/len(self.society.agents) > self.k_n_thr:\n",
        "            for i in range(len(socialLearner.preference_history)):\n",
        "                self.preference_history[i].append(socialLearner.preference_history[i])\n",
        "        individualLearner = self.individual(environment = env,id = 0)\n",
        "        for i in range(trial_cnt):\n",
        "            obs, Ri, index_selected_arm = individualLearner.take_action()\n",
        "            historyindividual.append(index_selected_arm)\n",
        "            historyindividualR.append(Ri)\n",
        "        self.historyindividual.append(historyindividual)\n",
        "        self.historyindividualR.append(historyindividualR)\n",
        "        self.society.historyRecord()\n",
        "        self.history.append(history)\n",
        "        self.historyR.append(historyR)\n",
        "    def loop(self, epoch = 10, iteration = 10, vars = [], rewards = [(i-100)/10 for i in range(200)], threshold = 1000, noize = 0, n = [100], reward_var = None, reward_means = None, random_reward = True, k = None, preferencelearner = None):\n",
        "        for i in range(len(self.society.historyTotal.keys())+1):\n",
        "            self.preference_history[i] = []\n",
        "        for (reward_means,reward_var) in zip(rewards, vars):\n",
        "            for i in range(iteration):               \n",
        "                env = NArmedBanditEnvironment(len(reward_means), reward_means, reward_var)\n",
        "                self.iteration(env, 1000, preferencelearner)\n",
        "                self.best.append(np.argmax(reward_means))\n",
        "                self.bestR.append(np.max(reward_means))\n",
        "    def optimalaction(self, hist):\n",
        "        poa = []\n",
        "        for i in range(len(hist)):\n",
        "            temp = [0]\n",
        "            for index_selected_arm in hist[i]:\n",
        "                temp.append(temp[-1]+int(index_selected_arm==self.best[i]))\n",
        "            temp = temp[1:]\n",
        "            for t in range(len(temp)):\n",
        "                temp[t] = temp[t]/(t+1)\n",
        "            poa.append(temp)\n",
        "        return poa\n",
        "    def regret(self, hist):\n",
        "        poa = []\n",
        "        for i in range(len(hist)):\n",
        "            temp = [((j+1)*self.bestR[i]-np.sum(hist[i][0:j])) for j in range(len(hist[i]))]\n",
        "            poa.append(temp)\n",
        "        return poa\n",
        "    def creward(self, hist):\n",
        "        poa = []\n",
        "        for i in range(len(hist)):\n",
        "            poa.append(hist[i])\n",
        "        return poa\n",
        "    def regretc(self, hist):\n",
        "        poa = []\n",
        "        for i in range(len(hist)):\n",
        "            temp = [(self.bestR[i]-hist[i][j]) for j in range(len(hist[i]))]\n",
        "            poa.append(temp)\n",
        "        return poa\n",
        "    def compare(self, all = False):\n",
        "        if all:\n",
        "            index = [i for i in range(len(self.society.agents))]\n",
        "        else:\n",
        "            index = self.society.getCategories()\n",
        "        data = {}\n",
        "        data[\"history action\"] = self.history\n",
        "        data[\"history reward\"] = self.historyR\n",
        "        data[\"index\"] = index\n",
        "        data[\"history individual action\"] = self.historyindividual\n",
        "        data[\"history individual reward\"] = self.historyindividualR\n",
        "        data[\"optimal action\"] = self.best\n",
        "        data[\"optimal action reward\"] = self.bestR\n",
        "        tmp = {}\n",
        "        tmp[\"social\"] = self.preference_history[self.id]\n",
        "        for i in index:\n",
        "            tmp[self.society.names[i]] = self.preference_history[i]\n",
        "        data[\"preference\"] = tmp\n",
        "        poa = {}\n",
        "        poa[\"social\"] = self.optimalaction(self.history)\n",
        "        poa[\"individual\"] = self.optimalaction(self.historyindividual)\n",
        "        for i in index:\n",
        "            poa[self.society.names[i]] = self.optimalaction(self.society.historyTotal[i])\n",
        "        data[\"percent of selecting the optimal action\"] = poa\n",
        "        poa = {}\n",
        "        poa[\"social\"] = self.regret(self.historyR)\n",
        "        poa[\"individual\"] = self.regret(self.historyindividualR)\n",
        "        data[\"regret\"] = poa\n",
        "        poa = {}\n",
        "        poa[\"social\"] = self.regretc(self.historyR)\n",
        "        poa[\"individual\"] = self.regretc(self.historyindividualR)\n",
        "        data[\"regretc\"] = poa\n",
        "        poa = {}\n",
        "        poa[\"social\"] = self.creward(self.historyR)\n",
        "        poa[\"individual\"] = self.creward(self.historyindividualR)\n",
        "        data[\"creward\"] = poa\n",
        "        return data"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "WT-G_muSJW2U",
        "TweQfzExrl2h",
        "6BJbW-zkPpy4"
      ],
      "authorship_tag": "ABX9TyOsPozcjDRTGGNsMnMDUiDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}